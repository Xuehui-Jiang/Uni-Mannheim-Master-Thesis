{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1pWwg0lUrP9s-zrDMFM-BH8cPO70WXlYm",
      "authorship_tag": "ABX9TyOOD+03bXXjZ9K0yzgy24p0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xuehui-Jiang/Uni-Mannheim-Master-Thesis/blob/main/Experiment_Cities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb2PgGj_6BfA",
        "outputId": "94a7aa1f-0294-477a-8d60-b097e197a570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyRDF2Vec'...\n",
            "remote: Enumerating objects: 7396, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (220/220), done.\u001b[K\n",
            "remote: Total 7396 (delta 221), reused 261 (delta 137), pack-reused 7032\u001b[K\n",
            "Receiving objects: 100% (7396/7396), 5.32 MiB | 11.29 MiB/s, done.\n",
            "Resolving deltas: 100% (5022/5022), done.\n"
          ]
        }
      ],
      "source": [
        "# Cloning the pyRDF2Vec repo\n",
        "!git clone https://github.com/IBCNServices/pyRDF2Vec.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall community\n",
        "!pip install python-louvain\n",
        "!pip install pyRDF2Vec --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLxBCxW86fxd",
        "outputId": "d196b9dc-0552-49b6-9a5d-40996e2f1bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: community 1.0.0b1\n",
            "Uninstalling community-1.0.0b1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/community-1.0.0b1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/community/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.10/dist-packages/community/community_louvain.py\n",
            "    /usr/local/lib/python3.10/dist-packages/community/community_status.py\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled community-1.0.0b1\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from python-louvain) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-louvain) (1.23.5)\n",
            "Collecting pyRDF2Vec\n",
            "  Downloading pyrdf2vec-0.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs<22.0.0,>=21.2.0 (from pyRDF2Vec)\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<5.0.0,>=4.2.2 (from pyRDF2Vec)\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from pyRDF2Vec) (3.7.1)\n",
            "Collecting networkx<3.0.0,>=2.5.1 (from pyRDF2Vec)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-Levenshtein<0.13.0,>=0.12.2 (from pyRDF2Vec)\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-louvain<0.16,>=0.15 (from pyRDF2Vec)\n",
            "  Downloading python-louvain-0.15.tar.gz (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tomlkit<0.8.0,>=0.7.2 (from pyRDF2Vec)\n",
            "  Downloading tomlkit-0.7.2-py2.py3-none-any.whl (32 kB)\n",
            "Collecting torch<2.0.0,>=1.8.1 (from pyRDF2Vec)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.61.0 in /usr/local/lib/python3.10/dist-packages (from pyRDF2Vec) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein<0.13.0,>=0.12.2->pyRDF2Vec) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0,>=1.8.1->pyRDF2Vec) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.8.1->pyRDF2Vec) (0.41.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.16.0)\n",
            "Building wheels for collected packages: python-Levenshtein, python-louvain\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=159968 sha256=81f78b9b75070196d19b73b5b0fcd13cba5482cffe61f71906d6037ffd3fec82\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/c3/05/60b4747cf52e0f6b6ee52022088a4de07d755016493e86373d\n",
            "  Building wheel for python-louvain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-louvain: filename=python_louvain-0.15-py3-none-any.whl size=9398 sha256=c4f56facce9702047ea9531a29bede706c7450b9e3d73cd47b98b8b75ea11a87\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/99/50/8dd2ba8bc9d968b50e021fe35b15b1ff23e8df38ff4d9f559d\n",
            "Successfully built python-Levenshtein python-louvain\n",
            "Installing collected packages: tomlkit, python-Levenshtein, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, networkx, cachetools, attrs, python-louvain, nvidia-cudnn-cu11, torch, pyRDF2Vec\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.1\n",
            "    Uninstalling cachetools-5.3.1:\n",
            "      Successfully uninstalled cachetools-5.3.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: python-louvain\n",
            "    Found existing installation: python-louvain 0.16\n",
            "    Uninstalling python-louvain-0.16:\n",
            "      Successfully uninstalled python-louvain-0.16\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jsonschema 4.19.0 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "referencing 0.30.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-21.4.0 cachetools-4.2.4 networkx-2.8.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyRDF2Vec-0.2.3 python-Levenshtein-0.12.2 python-louvain-0.15 tomlkit-0.7.2 torch-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "!pip install networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kwaVLDi6f0A",
        "outputId": "32ee0345-e759-4504-8974-df2d0d56d92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/531.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/531.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m522.2/531.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (2.8.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import functools\n",
        "import itertools\n",
        "from typing import List, Sequence, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rdflib\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import sys\n",
        "sys.path.append('pyRDF2Vec')\n",
        "sys.path.append('pyRDF2Vec/pyrdf2vec')"
      ],
      "metadata": {
        "id": "auH2JmUJ6f21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyrdf2vec import RDF2VecTransformer\n",
        "from pyrdf2vec.graphs import KG\n",
        "from pyrdf2vec.embedders import Word2Vec\n",
        "from pyrdf2vec.walkers import RandomWalker"
      ],
      "metadata": {
        "id": "86G7TN-V6f5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "T5ntuM116f8A",
        "outputId": "89d6b59d-e9a5-4e0b-99fe-3026836f4a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ae41f28-b23e-4cfd-be0f-fa75fc2f268e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ae41f28-b23e-4cfd-be0f-fa75fc2f268e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CompleteDataset.tsv to CompleteDataset.tsv\n",
            "User uploaded file \"CompleteDataset.tsv\" with length 20612 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load training and testing entities\n",
        "complete_df = pd.read_csv('CompleteDataset.tsv', sep='\\t')\n",
        "\n",
        "cities = complete_df['city_name'].tolist()\n",
        "entities = complete_df['DBpedia_URL'].tolist()\n",
        "labels = complete_df['label'].tolist()\n",
        "reg_targets = complete_df['rating'].tolist()"
      ],
      "metadata": {
        "id": "ebtZ6jY-6f-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to obtain the DBpedia knowledge graph related to the entities in the dataset, the first method is to use the SPARQL endpoint provided by DBpedia to execute the query, retrieving direct types from the ontology, as well as outgoing and ingoing edges."
      ],
      "metadata": {
        "id": "CNCi1_gLHDrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import requests"
      ],
      "metadata": {
        "id": "WoZuGqvWQDub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBpedia SPARQL endpoint base URL\n",
        "endpoint_url = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "# Split entities into smaller chunks for batch processing\n",
        "batch_size = 71\n",
        "num_batches = math.ceil(len(entities) / batch_size)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Iterate over batches\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min((batch_idx + 1) * batch_size, len(entities))\n",
        "    batch_entities = entities[start_idx:end_idx]\n",
        "\n",
        "    # Create a formatted string with these entities for use in the SPARQL query\n",
        "    batch_entities_str = ', '.join(f\"dbr:{entity.split('/')[-1]}\" for entity in batch_entities)\n",
        "\n",
        "    # Update the SPARQL query to use the batch_entities_str\n",
        "    sparql_query_batch = f\"\"\"\n",
        "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "\n",
        "    CONSTRUCT {{?x rdf:type ?tx . }} WHERE {{\n",
        "        {{?x rdf:type ?tx . FILTER (?x IN ({batch_entities_str})) FILTER (REGEX(?tx, \"http://dbpedia.org/ontology/\")) }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the SPARQL query for the batch\n",
        "    params[\"query\"] = sparql_query_batch\n",
        "    response = requests.get(endpoint_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    all_results.append(response.text)\n",
        "\n",
        "# Save all the results to an output.nt file\n",
        "with open(\"direct_types_cities.nt\", 'w') as f:\n",
        "    for result in all_results:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All results saved to direct_types_cities.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5w11_kZHUJQ",
        "outputId": "d03ca2f4-1b4d-466d-8045-9434ecbb2e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All results saved to direct_types_cities.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DBpedia SPARQL endpoint base URL\n",
        "endpoint_url = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "# Split entities into smaller chunks for batch processing\n",
        "batch_size = 71\n",
        "num_batches = math.ceil(len(entities) / batch_size)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# Iterate over batches\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min((batch_idx + 1) * batch_size, len(entities))\n",
        "    batch_entities = entities[start_idx:end_idx]\n",
        "\n",
        "    # Create a formatted string with these entities for use in the SPARQL query\n",
        "    batch_entities_str = ', '.join(f\"dbr:{entity.split('/')[-1]}\" for entity in batch_entities)\n",
        "\n",
        "    # Update the SPARQL query to use the batch_entities_str\n",
        "    sparql_query_batch = f\"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
        "\n",
        "    CONSTRUCT {{\n",
        "        ?x ?y ?z .\n",
        "        ?z rdf:type ?tz .\n",
        "    }}\n",
        "    WHERE {{\n",
        "        {{ ?x ?y ?z .\n",
        "        OPTIONAL {{ ?z rdf:type ?tz . FILTER (REGEX(?tz, \"http://dbpedia.org/ontology/\")) }}\n",
        "        ?y a owl:ObjectProperty .\n",
        "        FILTER (?x IN ({batch_entities_str}))\n",
        "        FILTER (!REGEX(?y, \"http://dbpedia.org/ontology/wiki\"))\n",
        "        FILTER (REGEX(?z, \"http://dbpedia.org/resource/\")) }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the SPARQL query for the batch\n",
        "    params[\"query\"] = sparql_query_batch\n",
        "    response = requests.get(endpoint_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    all_results.append(response.text)\n",
        "\n",
        "# Save all the results to an output.nt file\n",
        "with open(\"outgoing_edges_cities.nt\", 'w') as f:\n",
        "    for result in all_results:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All results saved to outgoing_edges_cities.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsQElIIBVecM",
        "outputId": "0cefa370-7e49-46d1-b457-01b2ffb28b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All results saved to outgoing_edges_cities.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "hV7bKA6xekin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBpedia SPARQL endpoint base URL\n",
        "endpoint_url = \"https://dbpedia.org/sparql\"\n",
        "\n",
        "# Split entities into smaller chunks for batch processing\n",
        "batch_size = 5\n",
        "num_batches = math.ceil(len(entities) / batch_size)\n",
        "\n",
        "all_results = []\n",
        "failed_batches = []\n",
        "\n",
        "max_retries = 3\n",
        "\n",
        "# Iterate over batches\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min((batch_idx + 1) * batch_size, len(entities))\n",
        "    batch_entities = entities[start_idx:end_idx]\n",
        "\n",
        "    # Create a formatted string with these entities for use in the SPARQL query\n",
        "    batch_entities_str = ', '.join(f\"dbr:{entity.split('/')[-1]}\" for entity in batch_entities)\n",
        "\n",
        "    # Update the SPARQL query to use the batch_entities_str\n",
        "    sparql_query_batch = f\"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
        "\n",
        "    CONSTRUCT {{\n",
        "        ?a ?r ?x .\n",
        "        ?a rdf:type ?ta .\n",
        "    }}\n",
        "    WHERE {{\n",
        "        {{ ?a ?r ?x .\n",
        "        ?r a owl:ObjectProperty .\n",
        "        FILTER (?x IN ({batch_entities_str})) }}\n",
        "        FILTER (!REGEX(?r, \"http://dbpedia.org/ontology/wiki\"))\n",
        "        FILTER (REGEX(?a, \"http://dbpedia.org/resource/\"))\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    retries = 0\n",
        "    success = False\n",
        "    while retries < max_retries and not success:\n",
        "        try:\n",
        "            # Execute the SPARQL query for the batch\n",
        "            params[\"query\"] = sparql_query_batch\n",
        "            response = requests.get(endpoint_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            all_results.append(response.text)\n",
        "            success = True\n",
        "        except:\n",
        "            retries += 1\n",
        "            time.sleep(2)  # Wait for 2 seconds before retrying\n",
        "\n",
        "    # If all retries failed, record the batch_idx to failed_batches\n",
        "    if not success:\n",
        "        failed_batches.append(batch_idx)\n",
        "\n",
        "# Save all the results to an output.nt file\n",
        "with open(\"ingoing_edges_cities.nt\", 'w') as f:\n",
        "    for result in all_results:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All results saved to ingoing_edges_cities.nt!\")\n",
        "if failed_batches:\n",
        "    print(f\"Failed batches: {failed_batches}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEVWi1SAVeO7",
        "outputId": "f5bf04ef-f190-405a-e8fe-fa912edd3e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All results saved to ingoing_edges_cities.nt!\n",
            "Failed batches: [0, 1, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 27, 31, 32, 40]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx in failed_batches:\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min((batch_idx + 1) * batch_size, len(entities))\n",
        "    failed_batch_entities = entities[start_idx:end_idx]\n",
        "\n",
        "    for entity in failed_batch_entities:\n",
        "        batch_entities_str = f\"dbr:{entity.split('/')[-1]}\"\n",
        "\n",
        "        sparql_query_single = f\"\"\"\n",
        "        PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "        PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
        "\n",
        "        CONSTRUCT {{\n",
        "            ?a ?r ?x .\n",
        "            ?a rdf:type ?ta .\n",
        "        }}\n",
        "        WHERE {{\n",
        "            {{ ?a ?r ?x .\n",
        "            ?r a owl:ObjectProperty .\n",
        "            FILTER (?x IN ({batch_entities_str})) }}\n",
        "            FILTER (!REGEX(?r, \"http://dbpedia.org/ontology/wiki\"))\n",
        "            FILTER (REGEX(?a, \"http://dbpedia.org/resource/\"))\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            params[\"query\"] = sparql_query_single\n",
        "            response = requests.get(endpoint_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            all_results.append(response.text)\n",
        "        except:\n",
        "            print(f\"Failed to query for individual entity: {entity}\")\n",
        "\n",
        "# Save the updated results to an output.nt file\n",
        "with open(\"extra_ingoing_edges_cities.nt\", 'w') as f:\n",
        "    for result in all_results:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All updated results saved to extra_ingoing_edges_cities.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDLRt9UAnywE",
        "outputId": "4f491bd4-0809-4c9e-c82b-c42ceeb2694d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to query for individual entity: http://dbpedia.org/resource/Vienna\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Brussels\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Paris\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Montreal\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Wellington\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/London\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Kobe\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Dublin\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Los_Angeles\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Singapore\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Detroit\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Buenos_Aires\n",
            "All updated results saved to extra_ingoing_edges_cities.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "failed_entities = [\n",
        "    \"http://dbpedia.org/resource/Vienna\",\n",
        "    \"http://dbpedia.org/resource/Brussels\",\n",
        "    \"http://dbpedia.org/resource/Paris\",\n",
        "    \"http://dbpedia.org/resource/Montreal\",\n",
        "    \"http://dbpedia.org/resource/Wellington\",\n",
        "    \"http://dbpedia.org/resource/London\",\n",
        "    \"http://dbpedia.org/resource/Kobe\",\n",
        "    \"http://dbpedia.org/resource/Dublin\",\n",
        "    \"http://dbpedia.org/resource/Los_Angeles\",\n",
        "    \"http://dbpedia.org/resource/Singapore\",\n",
        "    \"http://dbpedia.org/resource/Detroit\",\n",
        "    \"http://dbpedia.org/resource/Buenos_Aires\"\n",
        "]\n",
        "\n",
        "individual_results = []\n",
        "\n",
        "for entity in failed_entities:\n",
        "    entity_str = f\"dbr:{entity.split('/')[-1]}\"\n",
        "\n",
        "    sparql_query_single = f\"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
        "\n",
        "    CONSTRUCT {{\n",
        "        ?a ?r ?x .\n",
        "        ?a rdf:type ?ta .\n",
        "    }}\n",
        "    WHERE {{\n",
        "        {{ ?a ?r ?x .\n",
        "        ?r a owl:ObjectProperty .\n",
        "        FILTER (?x IN ({entity_str})) }}\n",
        "        FILTER (!REGEX(?r, \"http://dbpedia.org/ontology/wiki\"))\n",
        "        FILTER (REGEX(?a, \"http://dbpedia.org/resource/\"))\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        params[\"query\"] = sparql_query_single\n",
        "        response = requests.get(endpoint_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        individual_results.append(response.text)\n",
        "        print(f\"Successfully queried for entity: {entity}\")\n",
        "        time.sleep(20)  # Wait for 20 seconds before the next query\n",
        "    except:\n",
        "        print(f\"Failed to query for individual entity: {entity}\")\n",
        "\n",
        "# Save the individual results to an output.nt file\n",
        "with open(\"individual_ingoing_edges_cities.nt\", 'w') as f:\n",
        "    for result in individual_results:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All individual results saved to individual_ingoing_edges_cities.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNlLnYd4poqk",
        "outputId": "7841f5e8-33de-4f36-9a6e-232e5b1fc92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully queried for entity: http://dbpedia.org/resource/Vienna\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Brussels\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Paris\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Montreal\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Wellington\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/London\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Kobe\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Dublin\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Los_Angeles\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Singapore\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Detroit\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Buenos_Aires\n",
            "All individual results saved to individual_ingoing_edges_cities.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of entities that failed in the last run\n",
        "failed_again_entities = [\n",
        "    \"http://dbpedia.org/resource/Paris\",\n",
        "    \"http://dbpedia.org/resource/London\",\n",
        "    \"http://dbpedia.org/resource/Los_Angeles\",\n",
        "    \"http://dbpedia.org/resource/Detroit\"\n",
        "]\n",
        "\n",
        "individual_results_again = []\n",
        "\n",
        "for entity in failed_again_entities:\n",
        "    entity_str = f\"dbr:{entity.split('/')[-1]}\"\n",
        "\n",
        "    sparql_query_single = f\"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
        "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
        "\n",
        "    CONSTRUCT {{\n",
        "        ?a ?r ?x .\n",
        "        ?a rdf:type ?ta .\n",
        "    }}\n",
        "    WHERE {{\n",
        "        {{ ?a ?r ?x .\n",
        "        ?r a owl:ObjectProperty .\n",
        "        FILTER (?x IN ({entity_str})) }}\n",
        "        FILTER (!REGEX(?r, \"http://dbpedia.org/ontology/wiki\"))\n",
        "        FILTER (REGEX(?a, \"http://dbpedia.org/resource/\"))\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        params[\"query\"] = sparql_query_single\n",
        "        response = requests.get(endpoint_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        individual_results_again.append(response.text)\n",
        "        print(f\"Successfully queried for entity: {entity}\")\n",
        "        time.sleep(40)  # Increased wait time to 40 seconds before the next query\n",
        "    except:\n",
        "        print(f\"Failed to query for individual entity: {entity}\")\n",
        "\n",
        "# Append the new individual results to the previous file\n",
        "with open(\"individual_ingoing_edges_cities.nt\", 'a') as f:\n",
        "    for result in individual_results_again:\n",
        "        f.write(result)\n",
        "\n",
        "print(\"All additional individual results appended to individual_ingoing_edges_cities.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAlbK0E-q3li",
        "outputId": "5f4ba160-0ef8-45e2-f65b-ab4aa720a6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to query for individual entity: http://dbpedia.org/resource/Paris\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/London\n",
            "Successfully queried for entity: http://dbpedia.org/resource/Los_Angeles\n",
            "Failed to query for individual entity: http://dbpedia.org/resource/Detroit\n",
            "All additional individual results appended to individual_ingoing_edges_cities.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entities will not used in the experiments\n",
        "absent_entities = [\n",
        "    \"http://dbpedia.org/resource/Paris\",\n",
        "    \"http://dbpedia.org/resource/London\",\n",
        "    \"http://dbpedia.org/resource/Detroit\"\n",
        "]\n",
        "\n",
        "# Filter out absent entities\n",
        "filtered_entities = [e for e in entities if e not in absent_entities]\n",
        "filtered_labels = [label for entity, label in zip(entities, labels) if entity not in absent_entities]\n",
        "filtered_reg_targets = [reg_targets for entity, reg_targets in zip(entities, reg_targets) if entity not in absent_entities]"
      ],
      "metadata": {
        "id": "0ASX7ksjuS0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"direct_types_cities.nt\", \"outgoing_edges_cities.nt\", \"ingoing_edges_cities.nt\", \"extra_ingoing_edges_cities.nt\", \"individual_ingoing_edges_cities.nt\"]\n",
        "with open(\"cities.nt\", 'w') as outfile:\n",
        "    for fname in filenames:\n",
        "        with open(fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "print(\"All files have been merged into combined_graph.nt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py3qrthFuynC",
        "outputId": "9da24bf4-0429-4997-c34e-454dab3fa454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files have been merged into combined_graph.nt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "destination_path = \"/content/drive/My Drive/dbpedia_files\"\n",
        "\n",
        "shutil.copy(\"cities.nt\", destination_path)\n",
        "\n",
        "print(f\"File saved to {destination_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ThO_uMrIYbz",
        "outputId": "0e4c9e39-b60a-4049-aea7-7eb34c6317ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to /content/drive/My Drive/dbpedia_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second method, which is the method we finally adopted to obtain the knowledge graph, is to download important files directly. And then Read each file and retain only those triples that are neither datatype properties nor literals."
      ],
      "metadata": {
        "id": "aQ0bZHTkQkBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of files\n",
        "files_to_merge = [\n",
        "    \"instance_types_en.ttl\",\n",
        "    \"infobox_properties_en.ttl\",\n",
        "    \"article_categories_en.ttl\",\n",
        "    \"disambiguations_en.ttl\"\n",
        "]\n",
        "\n",
        "# Merge files\n",
        "with open(\"direct_merged_dbpedia.ttl\", \"w\") as outfile:\n",
        "    for file in files_to_merge:\n",
        "        with open(\"/content/drive/My Drive/dbpedia_files/\" + file, \"r\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "print(\"Files have been merged into merged.ttl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx0mEY-u6gBP",
        "outputId": "9f42fd1b-8a28-4328-f6ce-5a289e61082e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been merged into merged.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of files\n",
        "files_to_merge = [\n",
        "    \"mappingbased_objects_en.ttl\",\n",
        "    \"geonames_links_en.ttl\"\n",
        "]\n",
        "\n",
        "# Merge files\n",
        "with open(\"indirect_merged_dbpedia.ttl\", \"w\") as outfile:\n",
        "    for file in files_to_merge:\n",
        "        with open(\"/content/drive/My Drive/dbpedia_files/\" + file, \"r\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "print(\"Files have been merged into merged.ttl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShdH3nnU6gED",
        "outputId": "35df5937-a20a-437f-acb2-6e7a58ad2fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been merged into merged.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For files that describe core relationships and entity types, we choose to filter directly for triples related to entities in the Dataset.\n",
        "For files that may provide more context and indirect relationships, we take a recursive approach to get entities and relationships that are directly or indirectly related to the target entity。"
      ],
      "metadata": {
        "id": "5ZLtMnPsRMVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out entities and relationships directly related to Cities Dataset\n",
        "with open(\"direct_merged_dbpedia.ttl\", \"r\") as file, open(\"direct_cities_filtered_dbpedia.ttl\", \"w\") as out_file:\n",
        "    for line in file:\n",
        "        triple = line.strip().split()\n",
        "        if len(triple) > 2:\n",
        "            # Check if the subject or object of the triple is in our city URL list\n",
        "            if triple[0].strip('<>') in entities or triple[2].strip('<>') in entities:\n",
        "                out_file.write(line)"
      ],
      "metadata": {
        "id": "VlQx6gau6gGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out entities and relationships directly or indirectly related to Cities Dataset\n",
        "entities_related_to_cities = set(entities)\n",
        "new_entities = set(entities)\n",
        "iteration_count = 0\n",
        "\n",
        "while new_entities and iteration_count < 2:\n",
        "    temp_entities = set()\n",
        "    with open(\"indirect_merged_dbpedia.ttl\", \"r\") as file:\n",
        "        for line in file:\n",
        "            triple = line.strip().split()\n",
        "            if len(triple) > 2 and (triple[0].strip('<>') in new_entities or triple[2].strip('<>') in new_entities):\n",
        "                    temp_entities.add(triple[0].strip('<>'))\n",
        "                    temp_entities.add(triple[2].strip('<>'))\n",
        "\n",
        "    new_entities = temp_entities - entities_related_to_cities\n",
        "    entities_related_to_cities.update(new_entities)\n",
        "    iteration_count += 1\n",
        "\n",
        "# Finally, we can write all triples that are directly or indirectly related to entities in the `entities_related_to_cities` collection\n",
        "with open(\"indirect_merged_dbpedia.ttl\", \"r\") as file, open(\"indirect_cities_filtered_dbpedia.ttl\", \"w\") as out_file:\n",
        "    for line in file:\n",
        "        triple = line.strip().split()\n",
        "        if len(triple) > 2 and (triple[0].strip('<>') in entities_related_to_cities or triple[2].strip('<>') in entities_related_to_cities):\n",
        "            out_file.write(line)\n"
      ],
      "metadata": {
        "id": "aKpN7BQm6gJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of files\n",
        "files_to_merge = [\n",
        "    \"direct_cities_filtered_dbpedia.ttl\",\n",
        "    \"indirect_cities_filtered_dbpedia.ttl\"\n",
        "]\n",
        "\n",
        "# Merge files\n",
        "with open(\"cities_filtered_dbpedia.ttl\", \"w\") as outfile:\n",
        "    for file in files_to_merge:\n",
        "        with open(file, \"r\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "print(\"Files have been merged into merged.ttl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2F01Aq36gLj",
        "outputId": "1138275f-9d53-4624-d70b-08d0161cc0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been merged into merged.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"cities_filtered_dbpedia.ttl\"\n",
        "output_file = \"cleaned_cities_filtered_dbpedia.ttl\"\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        # Determine whether it is a comment or a blank line\n",
        "        if line.startswith(\"#\") or line.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        # Split triples\n",
        "        triple = line.strip().split()\n",
        "\n",
        "        # Make sure this is a complete triplet\n",
        "        if len(triple) > 2:\n",
        "            # Check whether the object starts with \"http://dbpedia.org/resource/\"\n",
        "            if triple[2].startswith(\"<http://dbpedia.org/resource/\"):\n",
        "                outfile.write(line)\n",
        "\n",
        "print(f\"Finished filtering {input_file} and saved the result to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt1njh5X_ldD",
        "outputId": "5b1b7111-3b8b-4f91-d597-cde5f4e2380c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished filtering cities_filtered_dbpedia.ttl and saved the result to cleaned_cities_filtered_dbpedia.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 cleaned_cities_filtered_dbpedia.ttl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMuy1F7t6gOZ",
        "outputId": "56e2ee25-795e-43a2-cea2-9ba77bb1bd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<http://dbpedia.org/resource/Alberta> <http://dbpedia.org/property/largestcity> <http://dbpedia.org/resource/Calgary> .\n",
            "<http://dbpedia.org/resource/Azerbaijan> <http://dbpedia.org/property/largestCities> <http://dbpedia.org/resource/Baku> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/subdivisionType> <http://dbpedia.org/resource/Provinces_of_the_Netherlands> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/areaBlank1Title> <http://dbpedia.org/resource/Randstad> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/populationBlank1Title> <http://dbpedia.org/resource/Amsterdam_Metropolitan_Area> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/populationBlank2Title> <http://dbpedia.org/resource/Randstad> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/postalCodeType> <http://dbpedia.org/resource/Postal_codes_in_the_Netherlands> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/areaCodeType> <http://dbpedia.org/resource/Telephone_numbers_in_the_Netherlands> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/location> <http://dbpedia.org/resource/Amsterdam_Airport_Schiphol> .\n",
            "<http://dbpedia.org/resource/Amsterdam> <http://dbpedia.org/property/source> <http://dbpedia.org/resource/Royal_Netherlands_Meteorological_Institute> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "destination_path = \"/content/drive/My Drive/dbpedia_files\"\n",
        "\n",
        "shutil.copy(\"cleaned_cities_filtered_dbpedia.ttl\", destination_path)\n",
        "\n",
        "print(f\"File saved to {destination_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUHRJreX6gRj",
        "outputId": "3a46c684-4dce-4b82-b171-eb56463c53e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to /content/drive/My Drive/dbpedia_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"/content/drive/My Drive/dbpedia_files/cleaned_cities_filtered_dbpedia.ttl\", fmt='turtle')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "yXRvIiuqAweR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7NAQxIKAwdT",
        "outputId": "67880470-c6b3-416e-867e-6de615d6e3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://dbpedia.org/resource/Winston_Salem\n",
            "http://dbpedia.org/resource/Omuta\n",
            "http://dbpedia.org/resource/Rio_De_Janeiro\n",
            "http://dbpedia.org/resource/Port_Au_Prince\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entities not in KG\n",
        "absent_entities = [\n",
        "    \"http://dbpedia.org/resource/Winston_Salem\",\n",
        "    \"http://dbpedia.org/resource/Omuta\",\n",
        "    \"http://dbpedia.org/resource/Rio_De_Janeiro\",\n",
        "    \"http://dbpedia.org/resource/Port_Au_Prince\"\n",
        "    ]\n",
        "\n",
        "# Filter out absent entities\n",
        "filtered_entities = [e for e in entities if e not in absent_entities]\n",
        "filtered_labels = [label for entity, label in zip(entities, labels) if entity not in absent_entities]\n",
        "filtered_reg_targets = [reg_targets for entity, reg_targets in zip(entities, reg_targets) if entity not in absent_entities]"
      ],
      "metadata": {
        "id": "pth2uaOIVWL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "Izt1QV90wpLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare for Knowledge Graph"
      ],
      "metadata": {
        "id": "ikjIHBpMPwCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph, URIRef, Literal, Namespace\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "6vGSgnY2AwVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the graph from the input file\n",
        "graph = Graph()\n",
        "graph.parse(\"/content/drive/My Drive/dbpedia_files/cleaned_cities_filtered_dbpedia.ttl\", format=\"turtle\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mjZJ0DWAwUU",
        "outputId": "c664ca66-9102-496d-8331-e6c95fd53b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=N09f515e9152442eaa5abc12f75ff9dfb (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in graph:\n",
        "    G.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "qFuUmurtAwTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G.nodes())\n",
        "num_edges = len(G.edges())\n",
        "print(\"The DBpedia knowledge graph as of 2016-10 has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMPdfZyaAwLr",
        "outputId": "2631e1c5-7c30-45cb-de8b-3a9c853e6dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DBpedia knowledge graph as of 2016-10 has 4763333 nodes and 14631598 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Classification & Regression tasks based graph embeddings without SA"
      ],
      "metadata": {
        "id": "xEgubn0JQEuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "H5q9fRh6AwIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('ttl_class_reg_results.csv')"
      ],
      "metadata": {
        "id": "FlGI7MzEAwFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbF774XQQK9y",
        "outputId": "2dd20047-d6fa-4ddd-c80a-7c705411653c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.787143\n",
            "1           cbow         50                 C4.5  Accuracy   0.735476\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.792143\n",
            "3           cbow         50                  SVM  Accuracy   0.845000\n",
            "4           cbow         50    Linear Regression      RMSE  14.889069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('pure_class_reg_results.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-VL-JpO4T-wA",
        "outputId": "f30b57d8-0408-4d6a-f895-ba2847dbd961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_84e88f3e-318f-4619-a30f-9009c442ee35\", \"ttl_class_reg_results.csv\", 2016)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification & Regression tasks based graph embeddings extracted on subgraph base on SA with BFS"
      ],
      "metadata": {
        "id": "1EiKSJe4p66i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "WBnmmrC3qCg4",
        "outputId": "12ce3bff-adcb-401b-d50d-8ab458b8ac46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2e470a1-2868-432b-b944-e01815e89b0e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e2e470a1-2868-432b-b944-e01815e89b0e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving main.py to main.py\n",
            "Saving sa_helper.py to sa_helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from main import *\n",
        "from sa_helper import *"
      ],
      "metadata": {
        "id": "ZMRT079QqDO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the neighbor node number for each nodes in the graph\n",
        "neighbor_counts = {node: len(list(G.neighbors(node))) for node in G.nodes()}"
      ],
      "metadata": {
        "id": "5jAEbFPAqDNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "strict=True, excl=False, pop=False"
      ],
      "metadata": {
        "id": "FRL1mnpKVytc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_F_s = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.186, beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                    strict=True, fan_out=True, excl=False, pop=False)"
      ],
      "metadata": {
        "id": "h3vGVsM6qDD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_s = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_F_s:\n",
        "    G_sub_s.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "bflNVM5oq5NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_s.nodes())\n",
        "num_edges = len(G_sub_s.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfR4lMGuq5Lk",
        "outputId": "109780fb-5798-46e3-df5c-abaa86f65c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 946 nodes and 2431 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_F_s.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_s.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "vspd-7Jgq5A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_s.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "ZoeKxa8bq4p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG3yGRZCrwx8",
        "outputId": "fa003c8e-85db-4219-8aa1-3e9272b03303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://dbpedia.org/resource/Zurich\n",
            "http://dbpedia.org/resource/Dusseldorf\n",
            "http://dbpedia.org/resource/Nurnberg\n",
            "http://dbpedia.org/resource/Washington_Dc\n",
            "http://dbpedia.org/resource/Tsukuba\n",
            "http://dbpedia.org/resource/St._Peter_Port\n",
            "http://dbpedia.org/resource/Yokkaichi\n",
            "http://dbpedia.org/resource/Katsuyama\n",
            "http://dbpedia.org/resource/San_Jose\n",
            "http://dbpedia.org/resource/Brasilia\n",
            "http://dbpedia.org/resource/Noumea\n",
            "http://dbpedia.org/resource/Johor_Baharu\n",
            "http://dbpedia.org/resource/Sao_Paulo\n",
            "http://dbpedia.org/resource/Asuncion\n",
            "http://dbpedia.org/resource/Bogota\n",
            "http://dbpedia.org/resource/Blantyre\n",
            "http://dbpedia.org/resource/Medellin\n",
            "http://dbpedia.org/resource/St._Petersburg\n",
            "http://dbpedia.org/resource/Madras\n",
            "http://dbpedia.org/resource/Yaounde\n",
            "http://dbpedia.org/resource/Dacca\n",
            "http://dbpedia.org/resource/Dar_Es_Salaam\n",
            "http://dbpedia.org/resource/Ndjamena\n",
            "http://dbpedia.org/resource/Pointe_Noire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entities not in KG\n",
        "absent_entities = [\n",
        "    \"http://dbpedia.org/resource/Zurich\",\n",
        "    \"http://dbpedia.org/resource/Dusseldorf\",\n",
        "    \"http://dbpedia.org/resource/Nurnberg\",\n",
        "    \"http://dbpedia.org/resource/Washington_Dc\",\n",
        "    \"http://dbpedia.org/resource/Tsukuba\",\n",
        "    \"http://dbpedia.org/resource/St._Peter_Port\",\n",
        "    \"http://dbpedia.org/resource/Yokkaichi\",\n",
        "    \"http://dbpedia.org/resource/Katsuyama\",\n",
        "    \"http://dbpedia.org/resource/San_Jose\",\n",
        "    \"http://dbpedia.org/resource/Brasilia\",\n",
        "    \"http://dbpedia.org/resource/Noumea\",\n",
        "    \"http://dbpedia.org/resource/Johor_Baharu\",\n",
        "    \"http://dbpedia.org/resource/Sao_Paulo\",\n",
        "    \"http://dbpedia.org/resource/Asuncion\",\n",
        "    \"http://dbpedia.org/resource/Bogota\",\n",
        "    \"http://dbpedia.org/resource/Blantyre\",\n",
        "    \"http://dbpedia.org/resource/Medellin\",\n",
        "    \"http://dbpedia.org/resource/St._Petersburg\",\n",
        "    \"http://dbpedia.org/resource/Madras\",\n",
        "    \"http://dbpedia.org/resource/Yaounde\",\n",
        "    \"http://dbpedia.org/resource/Dacca\",\n",
        "    \"http://dbpedia.org/resource/Dar_Es_Salaam\",\n",
        "    \"http://dbpedia.org/resource/Ndjamena\",\n",
        "    \"http://dbpedia.org/resource/Pointe_Noire\"\n",
        "]\n",
        "\n",
        "# Filter out absent entities\n",
        "fil_filtered_entities = [e for e in filtered_entities if e not in absent_entities]\n",
        "fil_filtered_labels = [label for entity, label in zip(filtered_entities, labels) if entity not in absent_entities]\n",
        "fil_filtered_reg_targets = [reg_targets for entity, reg_targets in zip(filtered_entities, reg_targets) if entity not in absent_entities]"
      ],
      "metadata": {
        "id": "JZOXz_cIrwg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, fil_filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, fil_filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, fil_filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results_s.csv')"
      ],
      "metadata": {
        "id": "vFAaE0P8rwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJEZBqB69-sQ",
        "outputId": "ed8bea3d-80f9-4d40-8e7a-08c0405d602e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.411988\n",
            "1           cbow         50                 C4.5  Accuracy   0.544152\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.521345\n",
            "3           cbow         50                  SVM  Accuracy   0.553509\n",
            "4           cbow         50    Linear Regression      RMSE  28.118587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "strict=False, excl=False, pop=False"
      ],
      "metadata": {
        "id": "LOGhYqMJ-Fpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_F = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.186, beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=False, pop=False)"
      ],
      "metadata": {
        "id": "9FruayYH-GuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_F:\n",
        "    G_sub.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "hanplmcc_rQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub.nodes())\n",
        "num_edges = len(G_sub.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9SQguYO_q5o",
        "outputId": "54eaccf7-d916-48da-f42c-1c1e2766f2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 1151814 nodes and 1418106 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_loose.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "wowVOzHn_qZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_loose.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "MMwgz545_7nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "FJvkwTzW_9CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results.csv')"
      ],
      "metadata": {
        "id": "oWVLqnmd_8le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vXeJiwGAWcP",
        "outputId": "08e26e44-0086-4f62-f496-473bd8ce438d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.782381\n",
            "1           cbow         50                 C4.5  Accuracy   0.632381\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.782143\n",
            "3           cbow         50                  SVM  Accuracy   0.835714\n",
            "4           cbow         50    Linear Regression      RMSE  16.260528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "other schemes"
      ],
      "metadata": {
        "id": "GMzjhuUJO8HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=False"
      ],
      "metadata": {
        "id": "6LwqsbD4UiM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_F = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.186, beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=True, pop=False)"
      ],
      "metadata": {
        "id": "O5a2VIeWPFYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tf = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_F:\n",
        "    G_sub_tf.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "ADXcjbW_PGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tf.nodes())\n",
        "num_edges = len(G_sub_tf.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhlIhkwkPHfQ",
        "outputId": "81131eb2-2da2-42f9-8192-ef2292549b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 1155807 nodes and 1427512 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_tf.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "w74ScHY0PG_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_tf.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "Pp68srZ0Qq-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "Wd8abykSQq8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results_tf.csv')"
      ],
      "metadata": {
        "id": "88P3a_22QqsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXpt9SsVRoFm",
        "outputId": "831c845d-15b4-4a7e-fe6f-14d4eac7ca50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.768333\n",
            "1           cbow         50                 C4.5  Accuracy   0.646429\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.787143\n",
            "3           cbow         50                  SVM  Accuracy   0.845476\n",
            "4           cbow         50    Linear Regression      RMSE  16.226600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=True"
      ],
      "metadata": {
        "id": "iG9ip2k6UsHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_T = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts, alpha=0.186,\n",
        "                                        beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=True)"
      ],
      "metadata": {
        "id": "o65UxDF0uSuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ft = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_T:\n",
        "    G_sub_ft.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "urKtp5bLuSrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ft.nodes())\n",
        "num_edges = len(G_sub_ft.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "a62I-YTjuSn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c674ec-ddfd-4b23-dd91-1e0f50a1a69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273098 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_ft.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "wu5LacyluShW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_ft.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "0fPU7HAyuSMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "EWQETZscv5QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results_ft.csv')"
      ],
      "metadata": {
        "id": "y0fO4HSUv5IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "ctco_wU3wUZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576f133a-cfd6-4b5a-e345-bd9f899f3893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.577143\n",
            "1           cbow         50                 C4.5  Accuracy   0.628095\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.723810\n",
            "3           cbow         50                  SVM  Accuracy   0.767857\n",
            "4           cbow         50    Linear Regression      RMSE  19.939133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=True"
      ],
      "metadata": {
        "id": "fRjdNevqViiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_T = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts, alpha=0.186,\n",
        "                                        beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=True, pop=True)"
      ],
      "metadata": {
        "id": "zWnT7_GUL_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tt = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_T:\n",
        "    G_sub_tt.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "mdgAMfFQL_kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tt.nodes())\n",
        "num_edges = len(G_sub_tt.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "mv3rMlXcL_ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96f37f1-4406-4a43-c114-11fdb1399ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273098 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_tt.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "gRZCQkfKL_fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_tt.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "BbHATEytL_aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "4Fm45cZ0L_Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results_tt.csv')"
      ],
      "metadata": {
        "id": "TzepjvrvL_VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "uCoCiymgMagL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092605bd-4c1f-44ed-826f-a726b3a74a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.605476\n",
            "1           cbow         50                 C4.5  Accuracy   0.700714\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.714286\n",
            "3           cbow         50                  SVM  Accuracy   0.791190\n",
            "4           cbow         50    Linear Regression      RMSE  19.313904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification & Regression tasks based graph embeddings extracted on subgraph base on SA with DFS"
      ],
      "metadata": {
        "id": "48kszGlwO-aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=False"
      ],
      "metadata": {
        "id": "ADqZ72xgV8k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_F = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=False)"
      ],
      "metadata": {
        "id": "dnN2S3-cOi-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ff = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_F:\n",
        "    G_sub_ff.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "gVlzbK8QQNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ff.nodes())\n",
        "num_edges = len(G_sub_ff.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "0w_vDFfhQNba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c11254-9465-442c-d67b-5183ce95377d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273097 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_ff.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "Q88FQUpDQNZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_ff.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "ncNoE_irQNRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "wzBU9z_gQNOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_ttl_class_reg_results_ff.csv')"
      ],
      "metadata": {
        "id": "2tt_UUR9QNLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "Wn4mAWRJQNHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe8a434-bcd0-4e7f-c3ca-cd45c82c9f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.561667\n",
            "1           cbow         50                 C4.5  Accuracy   0.604524\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.715238\n",
            "3           cbow         50                  SVM  Accuracy   0.762857\n",
            "4           cbow         50    Linear Regression      RMSE  20.010073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=False"
      ],
      "metadata": {
        "id": "WerDu-U5WkhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_F = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.1, beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=True, pop=False)"
      ],
      "metadata": {
        "id": "4pbx-TTUSEA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tf = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_F:\n",
        "    G_sub_tf.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "vPt7Cv-3SD9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tf.nodes())\n",
        "num_edges = len(G_sub_tf.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "HLvCvo8gSD6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e83c196-1504-40e0-c425-ea47d0781d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273097 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_tf.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "AGj4ESvZSD3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_tf.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "CrMjPOexSD0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "8ttWHAfjSDyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_ttl_class_reg_results_tf.csv')"
      ],
      "metadata": {
        "id": "c0i5L_S7SDvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "w0DIpzr-SDsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24ab2e0-7429-429c-a0fb-a07385634dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.600714\n",
            "1           cbow         50                 C4.5  Accuracy   0.613333\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.700952\n",
            "3           cbow         50                  SVM  Accuracy   0.734286\n",
            "4           cbow         50    Linear Regression      RMSE  19.643796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=True"
      ],
      "metadata": {
        "id": "kqhyJz-QW4SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_T = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=True)"
      ],
      "metadata": {
        "id": "bM40UiLJTD_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ft = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_T:\n",
        "    G_sub_ft.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "GMb8rTXnTD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ft.nodes())\n",
        "num_edges = len(G_sub_ft.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "k1ca5I3rTD5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c517d85-2a20-4b1b-eb29-4016cac4fee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273097 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_ft.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "k3hxJ3uuUTWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_ft.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "m3w1RqudUTTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "E9fL7xoNUTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_ttl_class_reg_results_ft.csv')"
      ],
      "metadata": {
        "id": "F3KytSTPUTOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "uRcrBzLBUTMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72999bb8-d924-471a-c363-0f36732d51aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.581667\n",
            "1           cbow         50                 C4.5  Accuracy   0.667619\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.733333\n",
            "3           cbow         50                  SVM  Accuracy   0.753095\n",
            "4           cbow         50    Linear Regression      RMSE  20.475899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=True"
      ],
      "metadata": {
        "id": "IBkKvTwAW7FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_T = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=True, pop=True)"
      ],
      "metadata": {
        "id": "YIdXLt0YUse-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tt = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_T:\n",
        "    G_sub_tt.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "Wsu3m-lUUscb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tt.nodes())\n",
        "num_edges = len(G_sub_tt.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "b_w-1EKKUsZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b2f32a-844b-4b67-bd76-1e487914d7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 273097 nodes and 311897 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_tt.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "MikfMzPfUsWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_tt.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "jprqgdzdUsTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "hoYcWlAwUsQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_ttl_class_reg_results_tt.csv')"
      ],
      "metadata": {
        "id": "121fQ7W4Vj_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "1NkIWIf_Vj9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b3d728-0ec5-41d6-c593-ca92f5561bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.586905\n",
            "1           cbow         50                 C4.5  Accuracy   0.584762\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.699762\n",
            "3           cbow         50                  SVM  Accuracy   0.767143\n",
            "4           cbow         50    Linear Regression      RMSE  19.078100\n"
          ]
        }
      ]
    }
  ]
}