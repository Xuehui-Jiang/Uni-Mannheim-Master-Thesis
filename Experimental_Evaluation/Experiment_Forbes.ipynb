{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1jirkVmDs5tTvupBcTkTPazrd9BxDKknq",
      "authorship_tag": "ABX9TyPUoDcu/utN1Pi65l9+/k37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xuehui-Jiang/Uni-Mannheim-Master-Thesis/blob/main/Experimental_Evaluation/Experiment_Forbes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwSTRKpMHcKh",
        "outputId": "e6eda650-7115-445b-b79a-57f1ce801d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyRDF2Vec'...\n",
            "remote: Enumerating objects: 7412, done.\u001b[K\n",
            "remote: Counting objects: 100% (380/380), done.\u001b[K\n",
            "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
            "remote: Total 7412 (delta 234), reused 259 (delta 133), pack-reused 7032\u001b[K\n",
            "Receiving objects: 100% (7412/7412), 5.32 MiB | 12.35 MiB/s, done.\n",
            "Resolving deltas: 100% (5038/5038), done.\n"
          ]
        }
      ],
      "source": [
        "# Cloning the pyRDF2Vec repo\n",
        "!git clone https://github.com/IBCNServices/pyRDF2Vec.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall community\n",
        "!pip install python-louvain\n",
        "!pip install pyRDF2Vec --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dER1d4dOIzVV",
        "outputId": "169d5738-0058-4aa4-94c7-df45eb1de95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: community 1.0.0b1\n",
            "Uninstalling community-1.0.0b1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/community-1.0.0b1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/community/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.10/dist-packages/community/community_louvain.py\n",
            "    /usr/local/lib/python3.10/dist-packages/community/community_status.py\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled community-1.0.0b1\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from python-louvain) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-louvain) (1.23.5)\n",
            "Collecting pyRDF2Vec\n",
            "  Downloading pyrdf2vec-0.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs<22.0.0,>=21.2.0 (from pyRDF2Vec)\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<5.0.0,>=4.2.2 (from pyRDF2Vec)\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from pyRDF2Vec) (3.7.1)\n",
            "Collecting networkx<3.0.0,>=2.5.1 (from pyRDF2Vec)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-Levenshtein<0.13.0,>=0.12.2 (from pyRDF2Vec)\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-louvain<0.16,>=0.15 (from pyRDF2Vec)\n",
            "  Downloading python-louvain-0.15.tar.gz (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tomlkit<0.8.0,>=0.7.2 (from pyRDF2Vec)\n",
            "  Downloading tomlkit-0.7.2-py2.py3-none-any.whl (32 kB)\n",
            "Collecting torch<2.0.0,>=1.8.1 (from pyRDF2Vec)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.61.0 in /usr/local/lib/python3.10/dist-packages (from pyRDF2Vec) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein<0.13.0,>=0.12.2->pyRDF2Vec) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0,>=1.8.1->pyRDF2Vec) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.8.1->pyRDF2Vec)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.8.1->pyRDF2Vec) (0.41.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.4.2->pyRDF2Vec) (1.16.0)\n",
            "Building wheels for collected packages: python-Levenshtein, python-louvain\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=159966 sha256=2290e3ff8fb9a47193f8ffa9ec01991f6a6bcbbe8e517326eaf211447b0e5f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/c3/05/60b4747cf52e0f6b6ee52022088a4de07d755016493e86373d\n",
            "  Building wheel for python-louvain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-louvain: filename=python_louvain-0.15-py3-none-any.whl size=9398 sha256=addec4aa108b7d6b87378731cd57c21e63ab68a54b527e75138e9f7822a0b945\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/99/50/8dd2ba8bc9d968b50e021fe35b15b1ff23e8df38ff4d9f559d\n",
            "Successfully built python-Levenshtein python-louvain\n",
            "Installing collected packages: tomlkit, python-Levenshtein, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, networkx, cachetools, attrs, python-louvain, nvidia-cudnn-cu11, torch, pyRDF2Vec\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.1\n",
            "    Uninstalling cachetools-5.3.1:\n",
            "      Successfully uninstalled cachetools-5.3.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: python-louvain\n",
            "    Found existing installation: python-louvain 0.16\n",
            "    Uninstalling python-louvain-0.16:\n",
            "      Successfully uninstalled python-louvain-0.16\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jsonschema 4.19.0 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "referencing 0.30.2 requires attrs>=22.2.0, but you have attrs 21.4.0 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-21.4.0 cachetools-4.2.4 networkx-2.8.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyRDF2Vec-0.2.3 python-Levenshtein-0.12.2 python-louvain-0.15 tomlkit-0.7.2 torch-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "!pip install networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8M-n7LoIzS9",
        "outputId": "ab7e751f-754b-4a95-86f0-6110b104edb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (2.8.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import functools\n",
        "import itertools\n",
        "from typing import List, Sequence, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rdflib\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import sys\n",
        "sys.path.append('pyRDF2Vec')\n",
        "sys.path.append('pyRDF2Vec/pyrdf2vec')"
      ],
      "metadata": {
        "id": "uQI9QxNfIzQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyrdf2vec import RDF2VecTransformer\n",
        "from pyrdf2vec.graphs import KG\n",
        "from pyrdf2vec.embedders import Word2Vec\n",
        "from pyrdf2vec.walkers import RandomWalker"
      ],
      "metadata": {
        "id": "h7LO3mC_IzNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1tURXgpmIzK_",
        "outputId": "e47716f5-1e2c-47bb-aee7-769cdd67234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd67d0ce-fcf6-4936-99f8-9bc3956fdb58\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd67d0ce-fcf6-4936-99f8-9bc3956fdb58\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving completedataset.tsv to completedataset.tsv\n",
            "User uploaded file \"completedataset.tsv\" with length 185776 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load training and testing entities\n",
        "complete_df = pd.read_csv('completedataset.tsv', sep='\\t')\n",
        "\n",
        "entities = complete_df['DBpedia_URI'].tolist()\n",
        "labels = complete_df['label'].tolist()\n",
        "reg_targets = complete_df['Market_Value'].tolist()"
      ],
      "metadata": {
        "id": "ekfkZPI2IzIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out entities and relationships directly related to Forbes Dataset\n",
        "with open(\"/content/drive/My Drive/dbpedia_files/direct_merged_dbpedia.ttl\", \"r\") as file, open(\"direct_Forbes_filtered_dbpedia.ttl\", \"w\") as out_file:\n",
        "    for line in file:\n",
        "        triple = line.strip().split()\n",
        "        if len(triple) > 2:\n",
        "            # Check if the subject or object of the triple is in our forbes URL list\n",
        "            if triple[0].strip('<>') in entities or triple[2].strip('<>') in entities:\n",
        "                out_file.write(line)"
      ],
      "metadata": {
        "id": "oVV6OsJSIzFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out entities and relationships directly or indirectly related to Cities Dataset\n",
        "entities_related_to_cities = set(entities)\n",
        "new_entities = set(entities)\n",
        "iteration_count = 0\n",
        "\n",
        "while new_entities and iteration_count < 2:\n",
        "    temp_entities = set()\n",
        "    with open(\"/content/drive/My Drive/dbpedia_files/indirect_merged_dbpedia.ttl\", \"r\") as file:\n",
        "        for line in file:\n",
        "            triple = line.strip().split()\n",
        "            if len(triple) > 2 and (triple[0].strip('<>') in new_entities or triple[2].strip('<>') in new_entities):\n",
        "                    temp_entities.add(triple[0].strip('<>'))\n",
        "                    temp_entities.add(triple[2].strip('<>'))\n",
        "\n",
        "    new_entities = temp_entities - entities_related_to_cities\n",
        "    entities_related_to_cities.update(new_entities)\n",
        "    iteration_count += 1\n",
        "\n",
        "# Finally, we can write all triples that are directly or indirectly related to entities in the `entities_related_to_cities` collection\n",
        "with open(\"/content/drive/My Drive/dbpedia_files/indirect_merged_dbpedia.ttl\", \"r\") as file, open(\"indirect_Forbes_filtered_dbpedia.ttl\", \"w\") as out_file:\n",
        "    for line in file:\n",
        "        triple = line.strip().split()\n",
        "        if len(triple) > 2 and (triple[0].strip('<>') in entities_related_to_cities or triple[2].strip('<>') in entities_related_to_cities):\n",
        "            out_file.write(line)\n"
      ],
      "metadata": {
        "id": "LAusMUUuIzC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of files\n",
        "files_to_merge = [\n",
        "    \"direct_Forbes_filtered_dbpedia.ttl\",\n",
        "    \"indirect_Forbes_filtered_dbpedia.ttl\"\n",
        "]\n",
        "\n",
        "# Merge files\n",
        "with open(\"Forbes_filtered_dbpedia.ttl\", \"w\") as outfile:\n",
        "    for file in files_to_merge:\n",
        "        with open(file, \"r\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "print(\"Files have been merged into merged.ttl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBNKIYOnIzAd",
        "outputId": "eaa5c89c-664d-4b1e-ce07-be9cd86135ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been merged into merged.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"Forbes_filtered_dbpedia.ttl\"\n",
        "output_file = \"cleaned_Forbes_filtered_dbpedia.ttl\"\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        # Determine whether it is a comment or a blank line\n",
        "        if line.startswith(\"#\") or line.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        # Split triples\n",
        "        triple = line.strip().split()\n",
        "\n",
        "        # Make sure this is a complete triplet\n",
        "        if len(triple) > 2:\n",
        "            # Check whether the object starts with \"http://dbpedia.org/resource/\"\n",
        "            if triple[2].startswith(\"<http://dbpedia.org/resource/\"):\n",
        "                outfile.write(line)\n",
        "\n",
        "print(f\"Finished filtering {input_file} and saved the result to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPrUoTBFIy9w",
        "outputId": "2622c8c5-a4b2-4f29-b0f8-917b29db333a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished filtering Forbes_filtered_dbpedia.ttl and saved the result to cleaned_Forbes_filtered_dbpedia.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 cleaned_Forbes_filtered_dbpedia.ttl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsbwl_3DIy7E",
        "outputId": "a03af1ef-9ee4-4aff-d45d-827cb5a57461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<http://dbpedia.org/resource/AOL> <http://dbpedia.org/property/founders> <http://dbpedia.org/resource/Marc_Seriff> .\n",
            "<http://dbpedia.org/resource/AOL> <http://dbpedia.org/property/founders> <http://dbpedia.org/resource/Steve_Case> .\n",
            "<http://dbpedia.org/resource/AOL> <http://dbpedia.org/property/founders> <http://dbpedia.org/resource/Jim_Kimsey> .\n",
            "<http://dbpedia.org/resource/Alan_Kay> <http://dbpedia.org/property/workplaces> <http://dbpedia.org/resource/Hewlett-Packard> .\n",
            "<http://dbpedia.org/resource/Arkansas> <http://dbpedia.org/property/flower> <http://dbpedia.org/resource/Apple> .\n",
            "<http://dbpedia.org/resource/Adobe_Systems> <http://dbpedia.org/property/tradedAs> <http://dbpedia.org/resource/NASDAQ-100> .\n",
            "<http://dbpedia.org/resource/Adobe_Systems> <http://dbpedia.org/property/tradedAs> <http://dbpedia.org/resource/S&P_500> .\n",
            "<http://dbpedia.org/resource/Accelerated_Graphics_Port> <http://dbpedia.org/property/inventName> <http://dbpedia.org/resource/Intel> .\n",
            "<http://dbpedia.org/resource/Akio_Morita> <http://dbpedia.org/property/company> <http://dbpedia.org/resource/Sony> .\n",
            "<http://dbpedia.org/resource/Active_Server_Pages> <http://dbpedia.org/property/owner> <http://dbpedia.org/resource/Microsoft> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# 定义要保存的文件路径\n",
        "destination_path = \"/content/drive/My Drive/dbpedia_files\"\n",
        "\n",
        "# 复制文件到Google Drive\n",
        "shutil.copy(\"cleaned_Forbes_filtered_dbpedia.ttl\", destination_path)\n",
        "\n",
        "print(f\"File saved to {destination_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Og0oWo_Iy4Z",
        "outputId": "d9a95763-4e5c-4f17-e591-0823e1d8aa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to /content/drive/My Drive/dbpedia_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"/content/drive/My Drive/dbpedia_files/cleaned_Forbes_filtered_dbpedia.ttl\", fmt='turtle')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "g6xsYSaAPjul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMbKWKrNIyy7",
        "outputId": "ddd6c26d-d7a5-4941-9952-253bc1fc6585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://dbpedia.org/resource/HSBC_Holdings\n",
            "http://dbpedia.org/resource/Mitsubishi_UFJ_Financial\n",
            "http://dbpedia.org/resource/Westpac_Banking_Group\n",
            "http://dbpedia.org/resource/Goldman_Sachs_Group\n",
            "http://dbpedia.org/resource/Ford_Motor\n",
            "http://dbpedia.org/resource/Ping_An_Insurance_Group\n",
            "http://dbpedia.org/resource/Nissan_Motor\n",
            "http://dbpedia.org/resource/Hyundai_Motor\n",
            "http://dbpedia.org/resource/Saudi_Basic_Industries\n",
            "http://dbpedia.org/resource/China_Life_Insurance\n",
            "http://dbpedia.org/resource/China_Minsheng_Banking\n",
            "http://dbpedia.org/resource/Mitsubishi_Corp\n",
            "http://dbpedia.org/resource/Cnooc\n",
            "http://dbpedia.org/resource/Hon_Hai_Precision\n",
            "http://dbpedia.org/resource/China_Shenhua_Energy\n",
            "http://dbpedia.org/resource/US_Bancorp\n",
            "http://dbpedia.org/resource/Nordea_Bank\n",
            "http://dbpedia.org/resource/China_Citic_Bank\n",
            "http://dbpedia.org/resource/Credit_Suisse_Group\n",
            "http://dbpedia.org/resource/Industrial_Bank\n",
            "http://dbpedia.org/resource/PTT_PCL\n",
            "http://dbpedia.org/resource/UniCredit_Group\n",
            "http://dbpedia.org/resource/Glencore_International\n",
            "http://dbpedia.org/resource/TNK-BP_Holding\n",
            "http://dbpedia.org/resource/Canadian_Imperial_Bank\n",
            "http://dbpedia.org/resource/Bank_of_New_York_Mellon\n",
            "http://dbpedia.org/resource/Legal_&_General_Group\n",
            "http://dbpedia.org/resource/Costco_Wholesale\n",
            "http://dbpedia.org/resource/ACE\n",
            "http://dbpedia.org/resource/East_Japan_Railway\n",
            "http://dbpedia.org/resource/Seven_&_I_Holdings\n",
            "http://dbpedia.org/resource/People's_Insurance_Company\n",
            "http://dbpedia.org/resource/Taiwan_Semiconductor\n",
            "http://dbpedia.org/resource/BCE\n",
            "http://dbpedia.org/resource/SEB\n",
            "http://dbpedia.org/resource/Svenska_Handelsbanken\n",
            "http://dbpedia.org/resource/Shinhan_Financial_Group\n",
            "http://dbpedia.org/resource/Woolworths\n",
            "http://dbpedia.org/resource/Heineken_Holding\n",
            "http://dbpedia.org/resource/Power_Corp_of_Canada\n",
            "http://dbpedia.org/resource/OMV_Group\n",
            "http://dbpedia.org/resource/China_Railway_Construction\n",
            "http://dbpedia.org/resource/China_Railway_Group\n",
            "http://dbpedia.org/resource/KBC_Group\n",
            "http://dbpedia.org/resource/Indian_Oil\n",
            "http://dbpedia.org/resource/HCA_Holdings\n",
            "http://dbpedia.org/resource/Saudi_Telecom\n",
            "http://dbpedia.org/resource/Baoshan_Iron_&_Steel\n",
            "http://dbpedia.org/resource/AMP\n",
            "http://dbpedia.org/resource/Woori_Finance_Holdings\n",
            "http://dbpedia.org/resource/FirstRand\n",
            "http://dbpedia.org/resource/PG&E\n",
            "http://dbpedia.org/resource/Deutsche_Lufthansa\n",
            "http://dbpedia.org/resource/SK_Holdings\n",
            "http://dbpedia.org/resource/Huaxia_Bank\n",
            "http://dbpedia.org/resource/QBE_Insurance_Group\n",
            "http://dbpedia.org/resource/Generali_Group\n",
            "http://dbpedia.org/resource/CLP_Holdings\n",
            "http://dbpedia.org/resource/Grupo_Mexico\n",
            "http://dbpedia.org/resource/Lincoln_National\n",
            "http://dbpedia.org/resource/Fifth_Third_Bancorp\n",
            "http://dbpedia.org/resource/Porsche_Automobil_Holding\n",
            "http://dbpedia.org/resource/Kirin_Holdings\n",
            "http://dbpedia.org/resource/Keppel_Corp\n",
            "http://dbpedia.org/resource/China_Coal_Energy\n",
            "http://dbpedia.org/resource/GFNorte\n",
            "http://dbpedia.org/resource/Dongfeng_Motor_Group\n",
            "http://dbpedia.org/resource/Tokio_Marine_Holdings\n",
            "http://dbpedia.org/resource/Qatar_National_Bank\n",
            "http://dbpedia.org/resource/J_Sainsbury\n",
            "http://dbpedia.org/resource/Anglo_American\n",
            "http://dbpedia.org/resource/CaixaBank\n",
            "http://dbpedia.org/resource/Korea_Electric_Power\n",
            "http://dbpedia.org/resource/Sumitomo_Electric\n",
            "http://dbpedia.org/resource/Regions_Financial\n",
            "http://dbpedia.org/resource/Precision_Castparts\n",
            "http://dbpedia.org/resource/Franklin_Resources\n",
            "http://dbpedia.org/resource/Swatch_Group\n",
            "http://dbpedia.org/resource/Yum_Brands\n",
            "http://dbpedia.org/resource/Hartford_Financial_Services\n",
            "http://dbpedia.org/resource/HJ_Heinz\n",
            "http://dbpedia.org/resource/Mitsubishi_Chemical\n",
            "http://dbpedia.org/resource/Datang_International_Power\n",
            "http://dbpedia.org/resource/Ensco\n",
            "http://dbpedia.org/resource/Evergrande_Real_Estate\n",
            "http://dbpedia.org/resource/Grupo_Aval\n",
            "http://dbpedia.org/resource/Daiwa_House_Industry\n",
            "http://dbpedia.org/resource/Gree_Electric_Appliances\n",
            "http://dbpedia.org/resource/Erste_Group_Bank\n",
            "http://dbpedia.org/resource/Yanzhou_Coal_Mining\n",
            "http://dbpedia.org/resource/Parker-Hannifin\n",
            "http://dbpedia.org/resource/Deutsche_Boerse\n",
            "http://dbpedia.org/resource/Metro_Group\n",
            "http://dbpedia.org/resource/Lenovo_Group\n",
            "http://dbpedia.org/resource/VakifBank\n",
            "http://dbpedia.org/resource/ALFA\n",
            "http://dbpedia.org/resource/KeyCorp\n",
            "http://dbpedia.org/resource/Anhui_Conch_Cement\n",
            "http://dbpedia.org/resource/Luxottica_Group\n",
            "http://dbpedia.org/resource/BRF-Brasil_Foods\n",
            "http://dbpedia.org/resource/Asahi_Glass\n",
            "http://dbpedia.org/resource/West_Japan_Railway\n",
            "http://dbpedia.org/resource/SCOR\n",
            "http://dbpedia.org/resource/Biogen_Idec\n",
            "http://dbpedia.org/resource/Kansai_Electric_Power\n",
            "http://dbpedia.org/resource/KGHM_Polska_Miedz\n",
            "http://dbpedia.org/resource/Citic_Pacific\n",
            "http://dbpedia.org/resource/IDGC_Holding\n",
            "http://dbpedia.org/resource/Citic_Securities\n",
            "http://dbpedia.org/resource/GD_Power_Development\n",
            "http://dbpedia.org/resource/Turk_Telekom\n",
            "http://dbpedia.org/resource/TRW_Automotive_Holdings\n",
            "http://dbpedia.org/resource/CF_Industries_Holdings\n",
            "http://dbpedia.org/resource/Tokyo_Electric_Power\n",
            "http://dbpedia.org/resource/Mazda_Motor\n",
            "http://dbpedia.org/resource/Aluminum_Corp_of_China\n",
            "http://dbpedia.org/resource/Saudi_British_Bank\n",
            "http://dbpedia.org/resource/Asustek_Computer\n",
            "http://dbpedia.org/resource/Zijin_Mining_Group\n",
            "http://dbpedia.org/resource/Suning_Appliance\n",
            "http://dbpedia.org/resource/Genuine_Parts\n",
            "http://dbpedia.org/resource/Uni-President\n",
            "http://dbpedia.org/resource/WW_Grainger\n",
            "http://dbpedia.org/resource/Campbell_Soup\n",
            "http://dbpedia.org/resource/Daiwa_Securities\n",
            "http://dbpedia.org/resource/Kweichow_Moutai\n",
            "http://dbpedia.org/resource/Antarchile\n",
            "http://dbpedia.org/resource/Jeronimo_Martins\n",
            "http://dbpedia.org/resource/Petronas_Chemicals\n",
            "http://dbpedia.org/resource/Electrolux_Group\n",
            "http://dbpedia.org/resource/Sinopharm_Group\n",
            "http://dbpedia.org/resource/Great_Wall_Motor\n",
            "http://dbpedia.org/resource/Ryanair_Holdings\n",
            "http://dbpedia.org/resource/GAIL_India\n",
            "http://dbpedia.org/resource/Eastman_Chemical\n",
            "http://dbpedia.org/resource/CH_Robinson_Worldwide\n",
            "http://dbpedia.org/resource/Land_Securities_Group\n",
            "http://dbpedia.org/resource/Latam_Airlines\n",
            "http://dbpedia.org/resource/Delta_Lloyd\n",
            "http://dbpedia.org/resource/UC_Rusal\n",
            "http://dbpedia.org/resource/Banca_MPS\n",
            "http://dbpedia.org/resource/Alleghany\n",
            "http://dbpedia.org/resource/Hyundai_Engineering\n",
            "http://dbpedia.org/resource/Shanghai_International_Port\n",
            "http://dbpedia.org/resource/St_Jude_Medical\n",
            "http://dbpedia.org/resource/China_Cosco_Holdings\n",
            "http://dbpedia.org/resource/Lorillard\n",
            "http://dbpedia.org/resource/Gjensidige_Forsikring\n",
            "http://dbpedia.org/resource/LeGrand\n",
            "http://dbpedia.org/resource/Shimao_Property_Holdings\n",
            "http://dbpedia.org/resource/Belle_International_Holdings\n",
            "http://dbpedia.org/resource/Flextronics_International\n",
            "http://dbpedia.org/resource/WR_Berkley\n",
            "http://dbpedia.org/resource/Pohjola_Bank\n",
            "http://dbpedia.org/resource/Hormel_Foods\n",
            "http://dbpedia.org/resource/T_Rowe_Price\n",
            "http://dbpedia.org/resource/Zimmer_Holdings\n",
            "http://dbpedia.org/resource/Brambles\n",
            "http://dbpedia.org/resource/Delek_Group\n",
            "http://dbpedia.org/resource/Colruyt\n",
            "http://dbpedia.org/resource/Chugoku_Electric_Power\n",
            "http://dbpedia.org/resource/Shoprite_Holdings\n",
            "http://dbpedia.org/resource/OHL_Group\n",
            "http://dbpedia.org/resource/Metalurgica_Gerdau\n",
            "http://dbpedia.org/resource/Hebei_Iron_&_Steel\n",
            "http://dbpedia.org/resource/Regeneron_Pharmaceuticals\n",
            "http://dbpedia.org/resource/Greentown_China_Holdings\n",
            "http://dbpedia.org/resource/GS_Holdings\n",
            "http://dbpedia.org/resource/Toppan_Printing\n",
            "http://dbpedia.org/resource/Metro_Inc\n",
            "http://dbpedia.org/resource/Mitsui_OSK_Lines\n",
            "http://dbpedia.org/resource/Qingdao_Haier\n",
            "http://dbpedia.org/resource/Orascom_Construction\n",
            "http://dbpedia.org/resource/Molson_Coors_Brewing\n",
            "http://dbpedia.org/resource/Synovus_Financial\n",
            "http://dbpedia.org/resource/Lennar\n",
            "http://dbpedia.org/resource/Qantas_Airways\n",
            "http://dbpedia.org/resource/TUI\n",
            "http://dbpedia.org/resource/DR_Horton\n",
            "http://dbpedia.org/resource/Family_Dollar_Stores\n",
            "http://dbpedia.org/resource/Mead_Johnson_Nutrition\n",
            "http://dbpedia.org/resource/Coca-Cola_HBC\n",
            "http://dbpedia.org/resource/Manila_Electric\n",
            "http://dbpedia.org/resource/Israel_Corp\n",
            "http://dbpedia.org/resource/Crown_Castle_International\n",
            "http://dbpedia.org/resource/China_Longyuan_Power\n",
            "http://dbpedia.org/resource/Zions_Bancorp\n",
            "http://dbpedia.org/resource/Wuhan_Iron_&_Steel\n",
            "http://dbpedia.org/resource/Eurasian_Natural_Resources\n",
            "http://dbpedia.org/resource/DSME\n",
            "http://dbpedia.org/resource/Supervalu\n",
            "http://dbpedia.org/resource/JGC\n",
            "http://dbpedia.org/resource/China_Taiping_Insurance\n",
            "http://dbpedia.org/resource/Scana\n",
            "http://dbpedia.org/resource/Rock-Tenn\n",
            "http://dbpedia.org/resource/Oriental_Land\n",
            "http://dbpedia.org/resource/Exxaro_Resources\n",
            "http://dbpedia.org/resource/Angang_Steel\n",
            "http://dbpedia.org/resource/Banco_BPI\n",
            "http://dbpedia.org/resource/Hengan_International_Group\n",
            "http://dbpedia.org/resource/SNC-Lavalin_Group\n",
            "http://dbpedia.org/resource/Hysan_Development\n",
            "http://dbpedia.org/resource/American_Water_Works\n",
            "http://dbpedia.org/resource/Yamaha_Motor\n",
            "http://dbpedia.org/resource/Petsmart\n",
            "http://dbpedia.org/resource/Nomos_Bank\n",
            "http://dbpedia.org/resource/Jindal_Steel_&_Power\n",
            "http://dbpedia.org/resource/BR_Malls\n",
            "http://dbpedia.org/resource/CR_Bard\n",
            "http://dbpedia.org/resource/Wistron\n",
            "http://dbpedia.org/resource/Kansas_City_Southern\n",
            "http://dbpedia.org/resource/Pinnacle_West\n",
            "http://dbpedia.org/resource/Enagas\n",
            "http://dbpedia.org/resource/Marfrig_Group\n",
            "http://dbpedia.org/resource/Plains_Exploration\n",
            "http://dbpedia.org/resource/77_Bank\n",
            "http://dbpedia.org/resource/Empresas_CMPC\n",
            "http://dbpedia.org/resource/Kawasaki_Kisen_Kaisha\n",
            "http://dbpedia.org/resource/Maanshan_Iron_&_Steel\n",
            "http://dbpedia.org/resource/Quinenco\n",
            "http://dbpedia.org/resource/TCL_Corp\n",
            "http://dbpedia.org/resource/NHPC\n",
            "http://dbpedia.org/resource/Netease\n",
            "http://dbpedia.org/resource/Serco_Group\n",
            "http://dbpedia.org/resource/Tiffany_&_Co\n",
            "http://dbpedia.org/resource/Bank_Muscat\n",
            "http://dbpedia.org/resource/Mashreq_Bank\n",
            "http://dbpedia.org/resource/Shanghai_Industrial\n",
            "http://dbpedia.org/resource/JC_Penney\n",
            "http://dbpedia.org/resource/Eutelsat_Communications\n",
            "http://dbpedia.org/resource/DIA\n",
            "http://dbpedia.org/resource/Doosan_Heavy_Industries\n",
            "http://dbpedia.org/resource/OGE_Energy\n",
            "http://dbpedia.org/resource/China_Agri-Industries\n",
            "http://dbpedia.org/resource/First_Niagara_Financial\n",
            "http://dbpedia.org/resource/Vesuvius\n",
            "http://dbpedia.org/resource/Toyo_Seikan_Kaisha\n",
            "http://dbpedia.org/resource/Telecom_of_New_Zealand\n",
            "http://dbpedia.org/resource/Chicago_Bridge_&_Iron\n",
            "http://dbpedia.org/resource/Total_Access_Communication\n",
            "http://dbpedia.org/resource/Laurentian_Bank\n",
            "http://dbpedia.org/resource/MISC\n",
            "http://dbpedia.org/resource/Kuala_Lumpur_Kepong\n",
            "http://dbpedia.org/resource/Mirvac_Group\n",
            "http://dbpedia.org/resource/Transurban_Group\n",
            "http://dbpedia.org/resource/Banque_Centrale_Populaire\n",
            "http://dbpedia.org/resource/Lindt_&_Sprungli\n",
            "http://dbpedia.org/resource/Symetra_Financial\n",
            "http://dbpedia.org/resource/Nippon_Meat_Packers\n",
            "http://dbpedia.org/resource/Whiting_Petroleum\n",
            "http://dbpedia.org/resource/Finning_International\n",
            "http://dbpedia.org/resource/Hakuhodo_DY_Holdings\n",
            "http://dbpedia.org/resource/Aeroflot-Russian_Airlines\n",
            "http://dbpedia.org/resource/First_Horizon_National\n",
            "http://dbpedia.org/resource/Wing_Hang_Bank\n",
            "http://dbpedia.org/resource/Tractor_Supply\n",
            "http://dbpedia.org/resource/UOL_Group\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entities not in KG\n",
        "absent_entities = [\n",
        "    \"http://dbpedia.org/resource/HSBC_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Mitsubishi_UFJ_Financial\",\n",
        "    \"http://dbpedia.org/resource/Westpac_Banking_Group\",\n",
        "    \"http://dbpedia.org/resource/Goldman_Sachs_Group\",\n",
        "    \"http://dbpedia.org/resource/Ford_Motor\",\n",
        "    \"http://dbpedia.org/resource/Ping_An_Insurance_Group\",\n",
        "    \"http://dbpedia.org/resource/Nissan_Motor\",\n",
        "    \"http://dbpedia.org/resource/Hyundai_Motor\",\n",
        "    \"http://dbpedia.org/resource/Saudi_Basic_Industries\",\n",
        "    \"http://dbpedia.org/resource/China_Life_Insurance\",\n",
        "    \"http://dbpedia.org/resource/China_Minsheng_Banking\",\n",
        "    \"http://dbpedia.org/resource/Mitsubishi_Corp\",\n",
        "    \"http://dbpedia.org/resource/Cnooc\",\n",
        "    \"http://dbpedia.org/resource/Hon_Hai_Precision\",\n",
        "    \"http://dbpedia.org/resource/China_Shenhua_Energy\",\n",
        "    \"http://dbpedia.org/resource/US_Bancorp\",\n",
        "    \"http://dbpedia.org/resource/Nordea_Bank\",\n",
        "    \"http://dbpedia.org/resource/China_Citic_Bank\",\n",
        "    \"http://dbpedia.org/resource/Credit_Suisse_Group\",\n",
        "    \"http://dbpedia.org/resource/Industrial_Bank\",\n",
        "    \"http://dbpedia.org/resource/PTT_PCL\",\n",
        "    \"http://dbpedia.org/resource/UniCredit_Group\",\n",
        "    \"http://dbpedia.org/resource/Glencore_International\",\n",
        "    \"http://dbpedia.org/resource/TNK-BP_Holding\",\n",
        "    \"http://dbpedia.org/resource/Canadian_Imperial_Bank\",\n",
        "    \"http://dbpedia.org/resource/Bank_of_New_York_Mellon\",\n",
        "    \"http://dbpedia.org/resource/Legal_&_General_Group\",\n",
        "    \"http://dbpedia.org/resource/Costco_Wholesale\",\n",
        "    \"http://dbpedia.org/resource/ACE\",\n",
        "    \"http://dbpedia.org/resource/East_Japan_Railway\",\n",
        "    \"http://dbpedia.org/resource/Seven_&_I_Holdings\",\n",
        "    \"http://dbpedia.org/resource/People's_Insurance_Company\",\n",
        "    \"http://dbpedia.org/resource/Taiwan_Semiconductor\",\n",
        "    \"http://dbpedia.org/resource/BCE\",\n",
        "    \"http://dbpedia.org/resource/SEB\",\n",
        "    \"http://dbpedia.org/resource/Svenska_Handelsbanken\",\n",
        "    \"http://dbpedia.org/resource/Shinhan_Financial_Group\",\n",
        "    \"http://dbpedia.org/resource/Woolworths\",\n",
        "    \"http://dbpedia.org/resource/Heineken_Holding\",\n",
        "    \"http://dbpedia.org/resource/Power_Corp_of_Canada\",\n",
        "    \"http://dbpedia.org/resource/OMV_Group\",\n",
        "    \"http://dbpedia.org/resource/China_Railway_Construction\",\n",
        "    \"http://dbpedia.org/resource/China_Railway_Group\",\n",
        "    \"http://dbpedia.org/resource/KBC_Group\",\n",
        "    \"http://dbpedia.org/resource/Indian_Oil\",\n",
        "    \"http://dbpedia.org/resource/HCA_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Saudi_Telecom\",\n",
        "    \"http://dbpedia.org/resource/Baoshan_Iron_&_Steel\",\n",
        "    \"http://dbpedia.org/resource/AMP\",\n",
        "    \"http://dbpedia.org/resource/Woori_Finance_Holdings\",\n",
        "    \"http://dbpedia.org/resource/FirstRand\",\n",
        "    \"http://dbpedia.org/resource/PG&E\",\n",
        "    \"http://dbpedia.org/resource/Deutsche_Lufthansa\",\n",
        "    \"http://dbpedia.org/resource/SK_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Huaxia_Bank\",\n",
        "    \"http://dbpedia.org/resource/QBE_Insurance_Group\",\n",
        "    \"http://dbpedia.org/resource/Generali_Group\",\n",
        "    \"http://dbpedia.org/resource/CLP_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Grupo_Mexico\",\n",
        "    \"http://dbpedia.org/resource/Lincoln_National\",\n",
        "    \"http://dbpedia.org/resource/Fifth_Third_Bancorp\",\n",
        "    \"http://dbpedia.org/resource/Porsche_Automobil_Holding\",\n",
        "    \"http://dbpedia.org/resource/Kirin_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Keppel_Corp\",\n",
        "    \"http://dbpedia.org/resource/China_Coal_Energy\",\n",
        "    \"http://dbpedia.org/resource/GFNorte\",\n",
        "    \"http://dbpedia.org/resource/Dongfeng_Motor_Group\",\n",
        "    \"http://dbpedia.org/resource/Tokio_Marine_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Qatar_National_Bank\",\n",
        "    \"http://dbpedia.org/resource/J_Sainsbury\",\n",
        "    \"http://dbpedia.org/resource/Anglo_American\",\n",
        "    \"http://dbpedia.org/resource/CaixaBank\",\n",
        "    \"http://dbpedia.org/resource/Korea_Electric_Power\",\n",
        "    \"http://dbpedia.org/resource/Sumitomo_Electric\",\n",
        "    \"http://dbpedia.org/resource/Regions_Financial\",\n",
        "    \"http://dbpedia.org/resource/Precision_Castparts\",\n",
        "    \"http://dbpedia.org/resource/Franklin_Resources\",\n",
        "    \"http://dbpedia.org/resource/Swatch_Group\",\n",
        "    \"http://dbpedia.org/resource/Yum_Brands\",\n",
        "    \"http://dbpedia.org/resource/Hartford_Financial_Services\",\n",
        "    \"http://dbpedia.org/resource/HJ_Heinz\",\n",
        "    \"http://dbpedia.org/resource/Mitsubishi_Chemical\",\n",
        "    \"http://dbpedia.org/resource/Datang_International_Power\",\n",
        "    \"http://dbpedia.org/resource/Ensco\",\n",
        "    \"http://dbpedia.org/resource/Evergrande_Real_Estate\",\n",
        "    \"http://dbpedia.org/resource/Grupo_Aval\",\n",
        "    \"http://dbpedia.org/resource/Daiwa_House_Industry\",\n",
        "    \"http://dbpedia.org/resource/Gree_Electric_Appliances\",\n",
        "    \"http://dbpedia.org/resource/Erste_Group_Bank\",\n",
        "    \"http://dbpedia.org/resource/Yanzhou_Coal_Mining\",\n",
        "    \"http://dbpedia.org/resource/Parker-Hannifin\",\n",
        "    \"http://dbpedia.org/resource/Deutsche_Boerse\",\n",
        "    \"http://dbpedia.org/resource/Metro_Group\",\n",
        "    \"http://dbpedia.org/resource/Lenovo_Group\",\n",
        "    \"http://dbpedia.org/resource/VakifBank\",\n",
        "    \"http://dbpedia.org/resource/ALFA\",\n",
        "    \"http://dbpedia.org/resource/KeyCorp\",\n",
        "    \"http://dbpedia.org/resource/Anhui_Conch_Cement\",\n",
        "    \"http://dbpedia.org/resource/Luxottica_Group\",\n",
        "    \"http://dbpedia.org/resource/BRF-Brasil_Foods\",\n",
        "    \"http://dbpedia.org/resource/Asahi_Glass\",\n",
        "    \"http://dbpedia.org/resource/West_Japan_Railway\",\n",
        "    \"http://dbpedia.org/resource/SCOR\",\n",
        "    \"http://dbpedia.org/resource/Biogen_Idec\",\n",
        "    \"http://dbpedia.org/resource/Kansai_Electric_Power\",\n",
        "    \"http://dbpedia.org/resource/KGHM_Polska_Miedz\",\n",
        "    \"http://dbpedia.org/resource/Citic_Pacific\",\n",
        "    \"http://dbpedia.org/resource/IDGC_Holding\",\n",
        "    \"http://dbpedia.org/resource/Citic_Securities\",\n",
        "    \"http://dbpedia.org/resource/GD_Power_Development\",\n",
        "    \"http://dbpedia.org/resource/Turk_Telekom\",\n",
        "    \"http://dbpedia.org/resource/TRW_Automotive_Holdings\",\n",
        "    \"http://dbpedia.org/resource/CF_Industries_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Tokyo_Electric_Power\",\n",
        "    \"http://dbpedia.org/resource/Mazda_Motor\",\n",
        "    \"http://dbpedia.org/resource/Aluminum_Corp_of_China\",\n",
        "    \"http://dbpedia.org/resource/Saudi_British_Bank\",\n",
        "    \"http://dbpedia.org/resource/Asustek_Computer\",\n",
        "    \"http://dbpedia.org/resource/Zijin_Mining_Group\",\n",
        "    \"http://dbpedia.org/resource/Suning_Appliance\",\n",
        "    \"http://dbpedia.org/resource/Genuine_Parts\",\n",
        "    \"http://dbpedia.org/resource/Uni-President\",\n",
        "    \"http://dbpedia.org/resource/WW_Grainger\",\n",
        "    \"http://dbpedia.org/resource/Campbell_Soup\",\n",
        "    \"http://dbpedia.org/resource/Daiwa_Securities\",\n",
        "    \"http://dbpedia.org/resource/Kweichow_Moutai\",\n",
        "    \"http://dbpedia.org/resource/Antarchile\",\n",
        "    \"http://dbpedia.org/resource/Jeronimo_Martins\",\n",
        "    \"http://dbpedia.org/resource/Petronas_Chemicals\",\n",
        "    \"http://dbpedia.org/resource/Electrolux_Group\",\n",
        "    \"http://dbpedia.org/resource/Sinopharm_Group\",\n",
        "    \"http://dbpedia.org/resource/Great_Wall_Motor\",\n",
        "    \"http://dbpedia.org/resource/Ryanair_Holdings\",\n",
        "    \"http://dbpedia.org/resource/GAIL_India\",\n",
        "    \"http://dbpedia.org/resource/Eastman_Chemical\",\n",
        "    \"http://dbpedia.org/resource/CH_Robinson_Worldwide\",\n",
        "    \"http://dbpedia.org/resource/Land_Securities_Group\",\n",
        "    \"http://dbpedia.org/resource/Latam_Airlines\",\n",
        "    \"http://dbpedia.org/resource/Delta_Lloyd\",\n",
        "    \"http://dbpedia.org/resource/UC_Rusal\",\n",
        "    \"http://dbpedia.org/resource/Banca_MPS\",\n",
        "    \"http://dbpedia.org/resource/Alleghany\",\n",
        "    \"http://dbpedia.org/resource/Hyundai_Engineering\",\n",
        "    \"http://dbpedia.org/resource/Shanghai_International_Port\",\n",
        "    \"http://dbpedia.org/resource/St_Jude_Medical\",\n",
        "    \"http://dbpedia.org/resource/China_Cosco_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Lorillard\",\n",
        "    \"http://dbpedia.org/resource/Gjensidige_Forsikring\",\n",
        "    \"http://dbpedia.org/resource/LeGrand\",\n",
        "    \"http://dbpedia.org/resource/Shimao_Property_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Belle_International_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Flextronics_International\",\n",
        "    \"http://dbpedia.org/resource/WR_Berkley\",\n",
        "    \"http://dbpedia.org/resource/Pohjola_Bank\",\n",
        "    \"http://dbpedia.org/resource/Hormel_Foods\",\n",
        "    \"http://dbpedia.org/resource/T_Rowe_Price\",\n",
        "    \"http://dbpedia.org/resource/Zimmer_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Brambles\",\n",
        "    \"http://dbpedia.org/resource/Delek_Group\",\n",
        "    \"http://dbpedia.org/resource/Colruyt\",\n",
        "    \"http://dbpedia.org/resource/Chugoku_Electric_Power\",\n",
        "    \"http://dbpedia.org/resource/Shoprite_Holdings\",\n",
        "    \"http://dbpedia.org/resource/OHL_Group\",\n",
        "    \"http://dbpedia.org/resource/Metalurgica_Gerdau\",\n",
        "    \"http://dbpedia.org/resource/Hebei_Iron_&_Steel\",\n",
        "    \"http://dbpedia.org/resource/Regeneron_Pharmaceuticals\",\n",
        "    \"http://dbpedia.org/resource/Greentown_China_Holdings\",\n",
        "    \"http://dbpedia.org/resource/GS_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Toppan_Printing\",\n",
        "    \"http://dbpedia.org/resource/Metro_Inc\",\n",
        "    \"http://dbpedia.org/resource/Mitsui_OSK_Lines\",\n",
        "    \"http://dbpedia.org/resource/Qingdao_Haier\",\n",
        "    \"http://dbpedia.org/resource/Orascom_Construction\",\n",
        "    \"http://dbpedia.org/resource/Molson_Coors_Brewing\",\n",
        "    \"http://dbpedia.org/resource/Synovus_Financial\",\n",
        "    \"http://dbpedia.org/resource/Lennar\",\n",
        "    \"http://dbpedia.org/resource/Qantas_Airways\",\n",
        "    \"http://dbpedia.org/resource/TUI\",\n",
        "    \"http://dbpedia.org/resource/DR_Horton\",\n",
        "    \"http://dbpedia.org/resource/Family_Dollar_Stores\",\n",
        "    \"http://dbpedia.org/resource/Mead_Johnson_Nutrition\",\n",
        "    \"http://dbpedia.org/resource/Coca-Cola_HBC\",\n",
        "    \"http://dbpedia.org/resource/Manila_Electric\",\n",
        "    \"http://dbpedia.org/resource/Israel_Corp\",\n",
        "    \"http://dbpedia.org/resource/Crown_Castle_International\",\n",
        "    \"http://dbpedia.org/resource/China_Longyuan_Power\",\n",
        "    \"http://dbpedia.org/resource/Zions_Bancorp\",\n",
        "    \"http://dbpedia.org/resource/Wuhan_Iron_&_Steel\",\n",
        "    \"http://dbpedia.org/resource/Eurasian_Natural_Resources\",\n",
        "    \"http://dbpedia.org/resource/DSME\",\n",
        "    \"http://dbpedia.org/resource/Supervalu\",\n",
        "    \"http://dbpedia.org/resource/JGC\",\n",
        "    \"http://dbpedia.org/resource/China_Taiping_Insurance\",\n",
        "    \"http://dbpedia.org/resource/Scana\",\n",
        "    \"http://dbpedia.org/resource/Rock-Tenn\",\n",
        "    \"http://dbpedia.org/resource/Oriental_Land\",\n",
        "    \"http://dbpedia.org/resource/Exxaro_Resources\",\n",
        "    \"http://dbpedia.org/resource/Angang_Steel\",\n",
        "    \"http://dbpedia.org/resource/Banco_BPI\",\n",
        "    \"http://dbpedia.org/resource/Hengan_International_Group\",\n",
        "    \"http://dbpedia.org/resource/SNC-Lavalin_Group\",\n",
        "    \"http://dbpedia.org/resource/Hysan_Development\",\n",
        "    \"http://dbpedia.org/resource/American_Water_Works\",\n",
        "    \"http://dbpedia.org/resource/Yamaha_Motor\",\n",
        "    \"http://dbpedia.org/resource/Petsmart\",\n",
        "    \"http://dbpedia.org/resource/Nomos_Bank\",\n",
        "    \"http://dbpedia.org/resource/Jindal_Steel_&_Power\",\n",
        "    \"http://dbpedia.org/resource/BR_Malls\",\n",
        "    \"http://dbpedia.org/resource/CR_Bard\",\n",
        "    \"http://dbpedia.org/resource/Wistron\",\n",
        "    \"http://dbpedia.org/resource/Kansas_City_Southern\",\n",
        "    \"http://dbpedia.org/resource/Pinnacle_West\",\n",
        "    \"http://dbpedia.org/resource/Enagas\",\n",
        "    \"http://dbpedia.org/resource/Marfrig_Group\",\n",
        "    \"http://dbpedia.org/resource/Plains_Exploration\",\n",
        "    \"http://dbpedia.org/resource/77_Bank\",\n",
        "    \"http://dbpedia.org/resource/Empresas_CMPC\",\n",
        "    \"http://dbpedia.org/resource/Kawasaki_Kisen_Kaisha\",\n",
        "    \"http://dbpedia.org/resource/Maanshan_Iron_&_Steel\",\n",
        "    \"http://dbpedia.org/resource/Quinenco\",\n",
        "    \"http://dbpedia.org/resource/TCL_Corp\",\n",
        "    \"http://dbpedia.org/resource/NHPC\",\n",
        "    \"http://dbpedia.org/resource/Netease\",\n",
        "    \"http://dbpedia.org/resource/Serco_Group\",\n",
        "    \"http://dbpedia.org/resource/Tiffany_&_Co\",\n",
        "    \"http://dbpedia.org/resource/Bank_Muscat\",\n",
        "    \"http://dbpedia.org/resource/Mashreq_Bank\",\n",
        "    \"http://dbpedia.org/resource/Shanghai_Industrial\",\n",
        "    \"http://dbpedia.org/resource/JC_Penney\",\n",
        "    \"http://dbpedia.org/resource/Eutelsat_Communications\",\n",
        "    \"http://dbpedia.org/resource/DIA\",\n",
        "    \"http://dbpedia.org/resource/Doosan_Heavy_Industries\",\n",
        "    \"http://dbpedia.org/resource/OGE_Energy\",\n",
        "    \"http://dbpedia.org/resource/China_Agri-Industries\",\n",
        "    \"http://dbpedia.org/resource/First_Niagara_Financial\",\n",
        "    \"http://dbpedia.org/resource/Vesuvius\",\n",
        "    \"http://dbpedia.org/resource/Toyo_Seikan_Kaisha\",\n",
        "    \"http://dbpedia.org/resource/Telecom_of_New_Zealand\",\n",
        "    \"http://dbpedia.org/resource/Chicago_Bridge_&_Iron\",\n",
        "    \"http://dbpedia.org/resource/Total_Access_Communication\",\n",
        "    \"http://dbpedia.org/resource/Laurentian_Bank\",\n",
        "    \"http://dbpedia.org/resource/MISC\",\n",
        "    \"http://dbpedia.org/resource/Kuala_Lumpur_Kepong\",\n",
        "    \"http://dbpedia.org/resource/Mirvac_Group\",\n",
        "    \"http://dbpedia.org/resource/Transurban_Group\",\n",
        "    \"http://dbpedia.org/resource/Banque_Centrale_Populaire\",\n",
        "    \"http://dbpedia.org/resource/Lindt_&_Sprungli\",\n",
        "    \"http://dbpedia.org/resource/Symetra_Financial\",\n",
        "    \"http://dbpedia.org/resource/Nippon_Meat_Packers\",\n",
        "    \"http://dbpedia.org/resource/Whiting_Petroleum\",\n",
        "    \"http://dbpedia.org/resource/Finning_International\",\n",
        "    \"http://dbpedia.org/resource/Hakuhodo_DY_Holdings\",\n",
        "    \"http://dbpedia.org/resource/Aeroflot-Russian_Airlines\",\n",
        "    \"http://dbpedia.org/resource/First_Horizon_National\",\n",
        "    \"http://dbpedia.org/resource/Wing_Hang_Bank\",\n",
        "    \"http://dbpedia.org/resource/Tractor_Supply\",\n",
        "    \"http://dbpedia.org/resource/UOL_Group\"\n",
        "]\n",
        "\n",
        "# Filter out absent entities\n",
        "filtered_entities = [e for e in entities if e not in absent_entities]\n",
        "filtered_labels = [label for entity, label in zip(entities, labels) if entity not in absent_entities]\n",
        "filtered_reg_targets = [reg_targets for entity, reg_targets in zip(entities, reg_targets) if entity not in absent_entities]"
      ],
      "metadata": {
        "id": "17PG7gFsjc2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare for Knowledge Graph"
      ],
      "metadata": {
        "id": "HgdvJ7CEjkdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph, URIRef, Literal, Namespace\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "op0WJxxrjcz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the graph from the input file\n",
        "graph = Graph()\n",
        "graph.parse(\"/content/drive/My Drive/dbpedia_files/cleaned_Forbes_filtered_dbpedia.ttl\", format=\"turtle\")"
      ],
      "metadata": {
        "id": "nBimvbngjcxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22268fd2-4ece-4c62-b2e3-bb077c73460e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=N35826d3397fa47dea2f5fc0d56f779b8 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in graph:\n",
        "    G.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "MMMzijWYjcuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G.nodes())\n",
        "num_edges = len(G.edges())\n",
        "print(\"The DBpedia knowledge graph as of 2016-10 has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "jtFPTHRjjcWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172f963d-27d7-4ffc-8c54-a26cac135058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The DBpedia knowledge graph as of 2016-10 has 3976256 nodes and 11546003 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification & Regression tasks based graph embeddings without SA"
      ],
      "metadata": {
        "id": "0wwbTLk3j8qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "iG6-wiaXjcTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('pure_class_reg_results.csv')"
      ],
      "metadata": {
        "id": "yooEHgvVjcQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "yKmZGvCyjcNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eff79f8-8c82-4025-aaf0-838cc8ec36e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.563802\n",
            "1           cbow         50                 C4.5  Accuracy   0.517994\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.546582\n",
            "3           cbow         50                  SVM  Accuracy   0.524772\n",
            "4           cbow         50    Linear Regression      RMSE  31.780191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification & Regression tasks based graph embeddings extracted on subgraph base on SA with BFS"
      ],
      "metadata": {
        "id": "JngXWwP5Jsxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "JgPnVNUNjcJF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "213d55ef-534c-46e6-b78b-85498b4fff2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b67c0f7-258c-4803-8a81-bf646b62d577\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b67c0f7-258c-4803-8a81-bf646b62d577\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving main.py to main.py\n",
            "Saving sa_helper.py to sa_helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from main import *\n",
        "from sa_helper import *"
      ],
      "metadata": {
        "id": "sHf0NUhWjb1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the neighbor node number for each nodes in the graph\n",
        "neighbor_counts = {node: len(list(G.neighbors(node))) for node in G.nodes()}"
      ],
      "metadata": {
        "id": "RZGF_BOvJ2Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=False"
      ],
      "metadata": {
        "id": "JgKqAExTw7BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_F = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.186, beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=False, pop=False)"
      ],
      "metadata": {
        "id": "cOW1BDkJJ2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_F:\n",
        "    G_sub.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "HetMYdmEJ2Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub.nodes())\n",
        "num_edges = len(G_sub.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "VxtsMgZ1J2Cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6895c277-739e-4660-d87e-a140354f1efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33956 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_ff.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "Yln8hBAmJ1_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_ff.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "hru901RDJ19A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "JmJnGsbaKXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_ttl_class_reg_results_ff.csv')"
      ],
      "metadata": {
        "id": "CA3s0QbAKXYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "azUieXdPKXVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1b1820-24de-48fd-9cf6-5b0aa2ad7a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.525592\n",
            "1           cbow         50                 C4.5  Accuracy   0.568461\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.627028\n",
            "3           cbow         50                  SVM  Accuracy   0.599949\n",
            "4           cbow         50    Linear Regression      RMSE  29.178628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=False"
      ],
      "metadata": {
        "id": "vzTot75YxfCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_F = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.186, beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=True, pop=False)"
      ],
      "metadata": {
        "id": "AnghUTdKKXTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tf = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_F:\n",
        "    G_sub_tf.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "Lon0AzejKXQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tf.nodes())\n",
        "num_edges = len(G_sub_tf.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "f787LFGtKXOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed98a104-0d8f-4d1b-9585-ab606cf53218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33956 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_tf.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "vWy5iU54KXLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_tf.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "7hRizwk2KXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "9IVp_yfvLGi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_class_reg_results_tf.csv')"
      ],
      "metadata": {
        "id": "GnBpcqrkLGhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "ZulU9WrKLGb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139c1d98-097e-4446-c3e5-f9dd364a25f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.518051\n",
            "1           cbow         50                 C4.5  Accuracy   0.563927\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.614998\n",
            "3           cbow         50                  SVM  Accuracy   0.613465\n",
            "4           cbow         50    Linear Regression      RMSE  29.874605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=True"
      ],
      "metadata": {
        "id": "i3TWT0sUxxgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_T = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts, alpha=0.186,\n",
        "                                        beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=True)"
      ],
      "metadata": {
        "id": "cgU_jhweLGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ft = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_T:\n",
        "    G_sub_ft.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "YImourmGLGW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ft.nodes())\n",
        "num_edges = len(G_sub_ft.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "0Bk5p1hPLGUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c2e7d4-1aec-4ba5-a368-765730373f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33956 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_ft.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "oNWM6OZVLGR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_ft.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "ILxWtQq-LGMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "fc3dt6NKLsxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_class_reg_results_ft.csv')"
      ],
      "metadata": {
        "id": "i5aiQkNuLsvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "YHQ5dGi0Lsp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d005d48d-fce4-40d5-b6ae-78cc35a9f3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.520335\n",
            "1           cbow         50                 C4.5  Accuracy   0.556380\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.608943\n",
            "3           cbow         50                  SVM  Accuracy   0.615043\n",
            "4           cbow         50    Linear Regression      RMSE  28.618589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=True"
      ],
      "metadata": {
        "id": "CWKf_xOHyBM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_T = spreading_activation_BFS(graph, G, filtered_entities, neighbor_counts, alpha=0.186,\n",
        "                                        beta=0.006, max_hops=3, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=True, pop=True)"
      ],
      "metadata": {
        "id": "MkPTrZgYLsnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tt = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_T:\n",
        "    G_sub_tt.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "5G4kFlfFLsk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tt.nodes())\n",
        "num_edges = len(G_sub_tt.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "RcWHz3kmLsiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83514d81-8a7a-426a-a627-9e93afa1aeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33956 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('BFS_subgraph_tt.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "KGsAS2XkLsf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"BFS_subgraph_tt.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "pHXxzE49LsbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "42OkIQ4NLsYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('BFS_class_reg_results_tt.csv')"
      ],
      "metadata": {
        "id": "Sm8YA_sBLsVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "3vDhn685OUBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bb4bd2-c669-4c14-8246-eadb04d4103a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.524066\n",
            "1           cbow         50                 C4.5  Accuracy   0.587241\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.609700\n",
            "3           cbow         50                  SVM  Accuracy   0.591718\n",
            "4           cbow         50    Linear Regression      RMSE  28.810306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification & Regression tasks based graph embeddings extracted on subgraph base on SA with DFS"
      ],
      "metadata": {
        "id": "tQDwXpr9PJ60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=False"
      ],
      "metadata": {
        "id": "p9cgzOU5yctr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_F = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=False)"
      ],
      "metadata": {
        "id": "_XlHRS_VPMe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ff = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_F:\n",
        "    G_sub_ff.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "oIY4C6HCPMcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ff.nodes())\n",
        "num_edges = len(G_sub_ff.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "57qSe4skPMaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94dd636-327b-4cb4-e43a-e754ded7b4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33955 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_ff.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "m8VD_3tXPMXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_ff.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "DBKpbJT4PMSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "HrMW5V-9PMPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_class_reg_results_ff.csv')"
      ],
      "metadata": {
        "id": "QrxxEdeeRwE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "qh147BXQRwB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282f6f2d-3d57-4a30-8571-af4bd51cb460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.514291\n",
            "1           cbow         50                 C4.5  Accuracy   0.554802\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.615755\n",
            "3           cbow         50                  SVM  Accuracy   0.595443\n",
            "4           cbow         50    Linear Regression      RMSE  28.941277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=False"
      ],
      "metadata": {
        "id": "N0FjQGfEyi0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_F = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts,\n",
        "                                    alpha=0.1, beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                    strict=False, fan_out=True, excl=True, pop=False)"
      ],
      "metadata": {
        "id": "jBRWJdizRv_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tf = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_F:\n",
        "    G_sub_tf.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "N2A_QBSRRv8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tf.nodes())\n",
        "num_edges = len(G_sub_tf.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "JPq8H-fYSX3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e0bbd-aacd-42c0-fe3f-f05dc91e1fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33955 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_F.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_tf.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "0GaIr_ANSX0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_tf.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "Q31ogoTxSXyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "3r3QfZcASXvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_class_reg_results_tf.csv')"
      ],
      "metadata": {
        "id": "jk2Mw-z7S8_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "4LZnNZ7jS89c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d80b9c4-48e9-436c-8f17-1f50a71eb5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.514303\n",
            "1           cbow         50                 C4.5  Accuracy   0.566912\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.606704\n",
            "3           cbow         50                  SVM  Accuracy   0.598445\n",
            "4           cbow         50    Linear Regression      RMSE  28.757347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=False, pop=True"
      ],
      "metadata": {
        "id": "dHb08NZcypbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_F_T = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=False, pop=True)"
      ],
      "metadata": {
        "id": "frYBqxbVS86z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_ft = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_F_T:\n",
        "    G_sub_ft.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "TmYgmzEUS84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_ft.nodes())\n",
        "num_edges = len(G_sub_ft.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "1oZ1ln5XUWYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ed96c7-80cb-459e-87cb-3b39d637227c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33955 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_F_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_ft.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "_jO8ZmKmUWWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_ft.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "p1UzG_jdUWTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "OM-JP6-aUWQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_class_reg_results_ft.csv')"
      ],
      "metadata": {
        "id": "HkgZsgXvUWOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "id": "_lj4tHu-UxCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71698b2-41e3-4718-f43e-5f3d8784a788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.519560\n",
            "1           cbow         50                 C4.5  Accuracy   0.558573\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.614257\n",
            "3           cbow         50                  SVM  Accuracy   0.600706\n",
            "4           cbow         50    Linear Regression      RMSE  28.840317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excl=True, pop=True"
      ],
      "metadata": {
        "id": "bMaMdRUmytWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_T_T = spreading_activation_DFS(graph, G, filtered_entities, neighbor_counts, alpha=0.1,\n",
        "                                        beta=0.009, max_depth=2, extraction_threshold=0,\n",
        "                                        strict=False, fan_out=True, excl=True, pop=True)"
      ],
      "metadata": {
        "id": "g1-YOXeeUw_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_sub_tt = nx.DiGraph()  # Create a directed graph\n",
        "\n",
        "for s, p, o in subgraph_T_T:\n",
        "    G_sub_tt.add_edge(s, o, predicate=p)"
      ],
      "metadata": {
        "id": "tJPs2QQnVLt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exhibihate the size of subgraph：The number of nodes and edges in the subgraph.\n",
        "num_nodes = len(G_sub_tt.nodes())\n",
        "num_edges = len(G_sub_tt.edges())\n",
        "print(\"The subgraph of DBpedia based on SA has \" + str(num_nodes) + \" nodes and \" + str(num_edges) + \" edges.\")"
      ],
      "metadata": {
        "id": "2TmU6yqhVLrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df9bbd6-6fb4-4d1e-ca44-ce8eebbbdfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subgraph of DBpedia based on SA has 33955 nodes and 46633 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the graph to TTL format\n",
        "nt_data = subgraph_T_T.serialize(format='nt')\n",
        "\n",
        "# Save to a TTL file\n",
        "with open('DFS_subgraph_tt.nt', 'w') as output_file:\n",
        "    output_file.write(nt_data)"
      ],
      "metadata": {
        "id": "T34DEQ_9VLpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import fmt\n",
        "\n",
        "# Create a knowledge graph\n",
        "kg = KG(\"DFS_subgraph_tt.nt\", fmt='nt')\n",
        "\n",
        "kgentities = kg._entities"
      ],
      "metadata": {
        "id": "wURDv4nqVLmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in filtered_entities:\n",
        "    found = False\n",
        "    for ekg in kgentities:\n",
        "        s = ekg.name\n",
        "        if s==e:\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "FbvlZ4UTViJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the results dataframe\n",
        "results = pd.DataFrame(columns=['Embedding Mode', 'Dimensions', 'Classifier/Regressor', 'Metric', 'Score'])\n",
        "\n",
        "# Define the walker with depth 4 and 500 walks per entity\n",
        "walker = RandomWalker(4, 500, with_reverse=False)\n",
        "\n",
        "# Train the embeddings for different configurations\n",
        "for mode in ['cbow', 'sg']:\n",
        "    for dim in [50, 100, 200]:\n",
        "        # 1. Generate embeddings\n",
        "        sg = 1 if mode == 'sg' else 0\n",
        "        embedder = Word2Vec(sg=sg, window=5, negative=25, vector_size=dim, sample=0, ns_exponent=0.75, epochs=5)\n",
        "        transformer = RDF2VecTransformer(embedder=embedder, walkers=[walker])\n",
        "\n",
        "        embeddings,_ = transformer.fit_transform(kg, filtered_entities)\n",
        "\n",
        "        # 2. Perform classification and regression tasks\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        kf = KFold(n_splits=10)\n",
        "\n",
        "        # Classification\n",
        "        classifiers = {\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'C4.5': DecisionTreeClassifier(),\n",
        "            'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
        "            'SVM': GridSearchCV(SVC(), param_grid={'C': [10**-3, 10**-2, 0.1, 1, 10, 10**2, 10**3]})\n",
        "        }\n",
        "\n",
        "        for name, clf in classifiers.items():\n",
        "            scores = cross_val_score(clf, embeddings, filtered_labels, cv=skf, scoring='accuracy')\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'Accuracy', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "        # Regression\n",
        "        regressors = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'KNN (k=3) Regressor': KNeighborsRegressor(n_neighbors=3),\n",
        "            'M5Rules': DecisionTreeRegressor()\n",
        "        }\n",
        "\n",
        "        for name, reg in regressors.items():\n",
        "            rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "            scores = cross_val_score(reg, embeddings, filtered_reg_targets, cv=kf, scoring=rmse_scorer)\n",
        "            results = results.append({'Embedding Mode': mode, 'Dimensions': dim, 'Classifier/Regressor': name, 'Metric': 'RMSE', 'Score': scores.mean()}, ignore_index=True)\n",
        "\n",
        "# Save the results\n",
        "results.to_csv('DFS_class_reg_results_tt.csv')"
      ],
      "metadata": {
        "id": "0nsnOg88ViIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XsJ5p1QUHVa",
        "outputId": "586139c0-5986-4053-e5e2-b668c2958768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Embedding Mode Dimensions Classifier/Regressor    Metric      Score\n",
            "0           cbow         50          Naive Bayes  Accuracy   0.521839\n",
            "1           cbow         50                 C4.5  Accuracy   0.573644\n",
            "2           cbow         50            KNN (k=3)  Accuracy   0.622551\n",
            "3           cbow         50                  SVM  Accuracy   0.595466\n",
            "4           cbow         50    Linear Regression      RMSE  29.287309\n"
          ]
        }
      ]
    }
  ]
}